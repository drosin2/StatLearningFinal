---
title: "Stats Learning Final Project"
author: "Jack Rudnick, Luke Lorenz, Doug Rosin"
date: "4/10/2022"
output: html_document
---

#TO DO:  
-     
- Impute NA values (need to change categorical cols classified as numeric to factor)
- create random forests for all vars, then quantitative only (idealized/initial assesment)
- Compare our prediction of mortality against SOFA pred of mortality (% acc or sd error?)

**I. Introduction**

– What does your dataset look like? What are the observations? What are the variables and their
types (categorical vs. quantitative)?


– Why is this data important and/or interesting, either to you personally or to someone else?


– If there is any background information that I should be aware of, please include it here. For
instance, if you have a jargon-heavy dataset, make sure that you describe it in a way that a
layperson would understand.


– What are your main research questions?


```{r importing libs}
library(tidyverse)
library(randomForest)
library(iml)
library(FNN)
library(e1071) #svm
library(quantreg) #quant reg
```


```{r reading in dataset}
data <- read.csv("full_cohort_data.csv")
```

```{r convert to factors}
data <- data %>%
  mutate(gender_num = factor(gender_num,
                             levels = c(0,1),
                             labels = c("Female", "Male")),
         aline_flg = factor(aline_flg,
                             levels = c(0,1),
                             labels = c("Year", "No")),
         service_num = factor(service_num,
                              levels = c(0,1),
                              labels = c("MICU or FICU", "SICU")),
         icu_exp_flg = factor(icu_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         hosp_exp_flg = factor(hosp_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         day_28_flg = factor(day_28_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         censor_flg = factor(censor_flg,
                               levels = c(0,1),
                               labels = c("Death","Censored")),
         sepsis_flg = factor(sepsis_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         chf_flg = factor(chf_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         afib_flg = factor(afib_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         renal_flg = factor(renal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         liver_flg = factor(liver_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         copd_flg = factor(copd_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         cad_flg = factor(cad_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         stroke_flg = factor(stroke_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         mal_flg = factor(mal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         resp_flg = factor(resp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes"))
         )

```


```{r imputing NA values}
na_indicies <- which(is.na(data), arr.ind=TRUE)  


# Function to calculate the mode
Mode <- function(x) {
  xx <- unique(x)
  xx[which.max(tabulate(match(x, xx)))]
}


#replace the factor variables with the mode
for(i in 1:ncol(data)){
  for(j in 1:nrow(data)){
    if(class(data[[i]])== "numeric"){
      if (is.na(data[j,i])){
        data[j,i] <- median(as.numeric(unlist(data[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(data[j,i])){
        data[j,i]<-Mode(na.omit(data[,i]))
    }
  }
  }
}

```


```{r}
iterate <- function(proxMatrix){
  for (i in 1:nrow(na_indicies)){
  column_names <-colnames(data)
  # Storing indicies of the na value
  rowNum <- na_indicies[i,1]
  colNum <- na_indicies[i,-1]
  column_name <- column_names[colNum]
  # If the column contains numeric variables as opposed to categorical

  if(sapply(data[,colNum], class)[1] == "numeric"){
    # print("Entering col 5")
    #updtating NA val with imputed val across the column
    imputed_val <- sum(data[-rowNum,column_name] * proxMatrix[-rowNum,rowNum]/ sum(proxMatrix[-rowNum,rowNum]))
    data[rowNum,colNum] <- imputed_val
    
  }else{ #Contains categorical vars, not numeric
    # print("entering other col")
    #df to store proximity excluding the row of intrest
    df_factor <- data.frame(proximity = proxMatrix[-rowNum,rowNum],
                            class = data[-rowNum,column_name])
    
    val <- df_factor %>% #computing for mode
      group_by_at("class") %>%
      summarize(avg = mean(proximity))
    index <- which.max(val$avg)
    data[rowNum,column_name] <- val[index,1]
  }
}
  
  
}
```

```{r imputing na data}
for (i in 1:20){
  #Fitting random forest to predict censor + getting prox mtx
  rf <- randomForest(formula = censor_flg ~., 
                   data = data,
                   proximity = TRUE)
  pmtx <- data.frame(rf$proximity)
  
  iterate(pmtx)
  print(paste("iteration", i , "done"))
}
```


#LINEAR REGRESSION FOR ICU STAY

For predicting the length of stay in the icu
  
```{r mean, median for ICU stay}
mean(data$icu_los_day)
median(data$icu_los_day)
```


```{r mean, median, and mode for the length of ICU stay}
ggplot(data = data,
       aes(x = icu_los_day))+
  geom_histogram(binwidth = 0.5) +
  labs(x = "Length of Stay in the ICU (Days)")

```

```{r maybe facet wrap based on a variable of intrest?}
ggplot(data = data,
       aes(x = icu_los_day))+
  geom_histogram(binwidth = 0.4) +
  labs(x = "Length of Stay in the Hospital (Days)") +
  facet_grid(~service_num)
```
#Medical intensive care unit vs surgical intensive care unit



```{r boxplot for outliers}
#computing outliers
outlier_data <- data %>%
  mutate(outlier = icu_los_day > median(icu_los_day) + IQR(icu_los_day) * 1.5)

ggplot(data = outlier_data,
       aes(x = icu_los_day,
           y =  ""))+
  geom_boxplot() +
  #for better outlier visualization
  # geom_point(data = function(x) filter_(x, ~ outlier), position = 'jitter')
  labs(x = "Length of Stay in the ICU (Days)")
```
#When looking at this data, we can see we have a large distribution of stays (1776) around the 2-4 day mark, but there are also 214 outliers in the data. These two are equally important to distinguish, as long stays in the ICU are resource intensive and important for allocation with finite spaces.

```{r getting the percent of outliers}
count(outlier_data %>% filter(outlier == TRUE))

count(outlier_data %>% filter(outlier == TRUE)) / count(outlier_data %>% filter(outlier == FALSE)) *100
```
13.7 percent of our patients admitted into the ICU were outliers

#FIT LR model on non-outliers, or short stay, vs outliers, or long stay

```{r splitting into testing/training}
set.seed(1)

short_stay <- outlier_data %>% filter(outlier == FALSE)

n <- nrow(short_stay)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_short <- short_stay %>%
  slice(train_indices)
test_short <- short_stay %>%
  slice(-train_indices)

```
#Do we need to filter data for outliers before or after splitting into testing/training???
#Can also filter where the duration of stay is above XXX days?

```{r}
#count unique vars
sapply(lapply(data, unique), length)
```
we can see sepsis_flg only has one var, so we do not include it in our model fitting

```{r Linear regression model for shorter stay (on training)}

#Taking a linear model without sepsis, because that column has no unique values
m <- lm(formula = icu_los_day ~ ., data = subset(short_stay,
                                                 select = -sepsis_flg)) 
m
```

```{r making predictions}
pred_short_stay <- predict(m, newdata = test_short)
```

```{r compute MSE and RMSE}
mean( (test_short$icu_los_day - pred_short_stay)^2 ) # test MSE
sqrt( mean( (test_short$icu_los_day - pred_short_stay)^2 ) ) # test RMSE
```




```{r Linear regression model for longer stay (on training)}

set.seed(1)

long_stay <- outlier_data %>% filter(outlier == FALSE)

n <- nrow(long_stay)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_long <- long_stay %>%
  slice(train_indices)
test_long <- long_stay %>%
  slice(-train_indices)

```


```{r}
m2 <- lm(formula = icu_los_day ~ ., data = subset(test_long,
                                                 select = -sepsis_flg)) 
m2
```

```{r}
pred_long_stay <- predict(m2, newdata = test_long)
```

```{r compute MSE and RMSE}
mean( (test_long$icu_los_day - pred_long_stay)^2 ) # test MSE
sqrt( mean( (test_long$icu_los_day - pred_long_stay)^2 ) ) # test RMSE
```
#comparatively, these are two very small error rates. For a test observation, we will therefore calculate if it is an outlier as we did above, and filter that result down into an appropriate linear model of prediction.


#Due to the sheer amount of factors, it may be understandable that a linear model does not work in higher dimensions. To combat this, we attempt to utilize a support vector machine on the long and short stays



```{r brief quantile regression for observation of age on icu stay}

colors <- c("#ffe6e6", "#ffcccc", "#ff9999", "#ff6666", "#ff3333",
            "#ff0000", "#cc0000", "#b30000", "#800000", "#4d0000", "#000000")
multi_rqfit <- rq(icu_los_day ~ age, data = data, tau = seq(0, 1, by = 0.1))
plot(icu_los_day ~ age, data = data, pch = 16)
for (j in 1:ncol(multi_rqfit$coefficients)) {
    abline(coef(multi_rqfit)[, j], col = colors[j])
}
```
#obviously, as the age of the patient admitted goes up, the average predicted ICU stay will also increase. This is apparent across all quantiles with all linear coefficients.



```{r splitting testing/training again}
set.seed(1)

n <- nrow(data)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_rf <- data %>%
  slice(train_indices)
test_rf <- data %>%
  slice(-train_indices)

```


```{r}
updated_train_rf <- train_rf %>%
  dplyr::select(-hospital_los_day) #taking out length of stay in hospital???
#removing var with obvious covariance

rf <- randomForest(formula = icu_los_day ~., 
                   data = train_rf,
                   importance = TRUE)

rf_with_hospital <- randomForest(formula = icu_los_day ~., 
                   data = updated_train_rf,
                   importance = TRUE)
```

```{r}
pred_rf <- predict(rf_with_hospital, newdata = test_rf)
```

```{r}
mean( (test_rf$icu_los_day - pred_rf)^2 ) # test MSE
sqrt( mean( (test_rf$icu_los_day - pred_rf)^2 ) ) # test RMSE
```
#our rmse was a bit higher for the random forest, but we also didn't split apart into outliers

```{r}
varImpPlot(rf_with_hospital)
```
#arterial blood gas count seemed to be the most important predictor for predicting length of ICU stay, for some reason
#The length of stay in the hospital was a  close second. I was unsure wether to keep this in
An arterial blood gas analysis (ABG) measures the balance of oxygen and carbon dioxide in your blood to see how well your lungs are working

Removed 

```{r}
pred_obj <- Predictor$new(rf, data = data)
temp <- FeatureEffect$new(pred_obj, feature = "abg_count", method = "pdp")
plot(temp)
```

```{r}
pred_obj <- Predictor$new(rf, data = data)
temp <- FeatureEffect$new(pred_obj, feature = "resp_flg", method = "pdp")
plot(temp)
```
#increase in ICU stay for a respitory disease




**III. Discussion/Conclusion**

– What did you learn about your data from this project?
– What would be the next step if you or someone else were to continue analyzing this data set? In
answering this, you can assume that you have unlimited time and resources to gather more data.
– What are some of the limitations of your work? For example, maybe you think you need other
modeling tools, maybe you think there’s too much randomness to glean anything meaningful from
your data, etc.


**IV. Bibliography**
– Cite where you retrieved your data.
– If you did any background research on your topic (e.g., to figure out what a variable means or to
verify whether your results make sense), include those sources too.

