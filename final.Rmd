---
title: "Stats Learning Final Project"
author: "Luke Lorenz, Jack Rudnick, Doug Rosin"
date: "4/10/2022"
output: html_document
---

**Important note**
We have generated additional figures, and they will occasionally be referenced as (table 1), (figure 1), etc. We have attached them with this shareable link along with the submission of this r markdown. We have also attached a copy of the writing for our project report to allow for clarity if needed.


LINK TO FIGURES:
https://docs.google.com/document/d/1faJaVsXWgVLi9n3HjGOjaqsxnFp1svlvIo7nWw5WQa4/edit?usp=sharing

LINK TO PAPER:
https://docs.google.com/document/d/1mHWCP0GX16UAMDeEpcbnLZ1Ei40WG1Hv2fFJYZnW9lQ/edit?usp=sharing


**I. Introduction**

Intensive care units (ICU) provide complex and resource-intensive treatment for the sickest hospitalized patients. The need for critical care medicine has grown substantially over the past decade and has consumed a huge portion of the income in many countries worldwide (Vincent et al, 2014). In 2010, critical care units represented 13.2% of hospital costs, 4.1% of national health expenditures, and 0.72% of U.S. gross domestic product. Length of stay in the ICU is the primary predictor associated with ICU cost, as a small percentage of patients with a prolonged length of stay in the ICU could consume a large proportion (nearly 50%) of ICU resource use (Evans et al, 2018). Effective prediction tools are strongly needed by clinical staff to adequately manage patient and budget stress.

Intensive care unit (ICU) patient monitoring has the potential to reduce complications and morbidity, and to increase the quality of care by enabling hospitals to deliver higher-quality, cost-effective patient care, and improve the quality of medical services in the ICU (Alghantani et al 2021). Additionally, it is a patient-centered outcome and therefore of interest to families, ICU personnel, and managers (Peres et al 2021). The ICU is among the most resource intensive units in hospitals, so ICU surveillance has the potential to reduce morbidity and plan for improvement of care. There is therefore a demand for ICU monitoring that can provide guidance and report when adverse medical events are anticipated (Alghatani et al 2021). Currently, ICU length of stay is used retrospectively to evaluate ICU efficiency. Patient length of stay prediction at admission could help coordinate care, implement preventative measures, and better communicate with managers, families (Peres et al 2021). 

With prolonged patient stays in the ICU, the burden of care for critically ill patients is massive. Given the role of the Intensive Care Unit (ICU) in providing intensive and specialized nursing and medicinal care for critically ill patients and sustaining life during a period of life-threatening organ system insufficiency, it is unsurprising that ICU’s contain the highest mortality rate of any unit of the hospital. Each year, about 500,000 out of 4 million patients entered into Intensive Care Units (ICU) pass away. In cases of severe illness, patients have an extraordinarily difficult choice to make. Patients can undergo intensive care, described above, to make every attempt to save their life, or they can request palliative care, which provides end of life care for patients and their families. Since early and frequent conversations over end of life treatment plans for patients suffering from serious conditions leads to increased satisfaction and quality of life stemming from care consistent to the patient’s desire, many hospitals have begun implementing machine learning-informed algorithms derived from patient data to inform doctors of patients survival chances and spur these conversations. Predicting mortality in patients hospitalized in ICU is crucial for assessing severity of illness and adjudicating the value of novel treatments mentioned above, interventions and health care policies. Machine learning can utilize important baseline information that can be acquired in a timely manner and therefore serve as accurate predictors in our ICU length of stay calculation. When scaled to all patients that visit ICUs daily, massive amounts of data make it feasible to develop predictive models that can help with management.

A Penn Medicine study examining the effect of an electronic nudge to doctors flagging patients who were at a high risk of death and would benefit from conversations surrounding end of life goals found a significant increase in the prevalence of these conversations between patients with doctors who received the nudge, in comparison to patients with doctors who were in the control group.  Additionally, doctors in the test group still engaged in end of life conversations with patients who were flagged as low mortality risk about 75% more regularly than doctors in the control group.  Even in cases where patients are likely to survive, these conversations are important for doctors and patients to coordinate and plan the best care possible for the patient.  In Section III of this paper, we build models which, similar to Penn Medicine, predict end of life outcomes for patients.  We compare our models with Penn Medicine’s models in the discussion.

While patients are admitted to the ICU with significant, typically life threatening injuries, many  also enter with significant comorbidities. A patient admitted for a car crash injury may also unknowingly have kidney disease, which could result in complications in treatment or sub-par recovery. Adverse events and clinical complications are a major cause of mortality and poor outcomes in patients, and substantial effort has been made to improve their recognition (Tomašev 2018). Unfortunately, few predictors have found their way into routine clinical practice because they either lack effective sensitivity and specificity. For example, liver disease can only be positively diagnosed after extensive testing (ultrasound, CT scan, or MRI) or a liver biopsy (Mayo Clinic). These tests may not be performed rapidly upon a patient’s arrival in the ICU, if the attending physician decides to have them performed at all, but as established above, the identification of these conditions can be critical. One medical review of car crash victims noted that conditions (including other medical maladies) could result in missed injuries (Garcia). 

The relevancy of ICU patients with various additional conditions remains after being released from the ICU. A 2010 study that followed 478 ICU patients found that preexisting conditions were associated with the majority of cases that experienced diminished quality of life after being released from the ICU (Orwelius et al.). Our hope is to use easily gathered patient information to predict which patients have pre existing medical conditions. This information could help ICU doctors identify patients that might have complicating factors, and more deliberately order tests for those conditions to better inform their treatment.

We aim to address these motivating questions of mortality, ICU stay, and pre-ICU conditions through rigorous application of statistical learning models. Due to the muli-faceted nature of our dataset, numerous models should be used in tandem to produce robust and accurate classification. These approaches allow us to thoroughly assess clinical features most commonly associated with our questions of interest.


**II. Methods **

We used a 1,776 patient subset of the 26,870 patient MIMIC-II dataset, which was constructed . from the ICUs of Beth Israel Deaconess Medical Center between 2001 to 2008. The MIMIC-II (‘Medical Information Mart for Intensive Care’) dataset is a large, single-center database comprising information relating to patients admitted to critical care units at a large care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. Our subset of data contains 46 variables extracted from MIMIC-II, including demographics (e.g. age, weight), clinical observations collected during the first day of ICU stay (e.g. white blood cell count, heart rate), and outcomes (e.g. 28 day mortality and length of stay).

To clean our data, we imputed the median value for continuous numerical variables and the mode observation for factor variables in place of NA observations.  We then iterated over the dataset 20 times, constructing a random forest and used the resulting proximity to repeatedly impute the NA values for a more accurate estimate of the unknown data.  Finally, we removed certain variables with obvious covariance, including hosp_exp_flg, icu_exp_flg, day_28_flg from the data, as in many cases incidences of mortality were indicated in multiple categories.  We also removed mort_day_censored, since it was an extension of censor_flg, where observations under 730 indicate death and observations over 730 indicate survival.

#Our dataset below contains 1776 observations and 46 different columns:

aline_flg: IAC used (categorical, 1 = year, 0 = no)
icu_los_day: length of stay in ICU (days, quantitative)
hospital_los_day: length of stay in hospital (days, quantitative)
age: age at baseline (years, quantitative)
gender_num: patient gender (1 = male; 0=female)
weight_first: first weight, (kg, quantitative)
bmi: patient BMI, (quantitative)
sapsi_first: first SAPS I score (quantitative)
sofa_first: first SOFA score (quantitative)
service_unit: type of service unit (character: FICU, MICU, SICU)
service_num: service as a quantitative (categorical: 0 = MICU or FICU, 1 = SICU)
day_icu_intime: day of week of ICU admission (character)
day_icu_intime_num: day of week of ICU admission (quantitative, corresponds with day_icu_intime)
hour_icu_intime: hour of ICU admission (quantitative, hour of admission using 24hr clock)
hosp_exp_flg: death in hospital (categorical: 1 = yes, 0 = no)
icu_exp_flg: death in ICU (categorical: 1 = yes, 0 = no)
day_28_flg: death within 28 days (categorical: 1 = yes, 0 = no)
mort_day_censored: day post ICU admission of censoring or death (days, quantitative)
censor_flg: censored or death (categorical: 0 = death, 1 = censored)
sepsis_flg: sepsis present (categorical: 0 = no, 1 = yes -- absent (0) for all)
chf_flg: Congestive heart failure (categorical: 0 = no, 1 = yes)
afib_flg: Atrial fibrillation (categorical: 0 = no, 1 = yes)
renal_flg: Chronic renal disease (categorical: 0 = no, 1 = yes)
liver_flg: Liver Disease (categorical: 0 = no, 1 = yes)
copd_flg: Chronic obstructive pulmonary disease (categorical: 0 = no, 1 = yes)
cad_flg: Coronary artery disease (categorical: 0 = no, 1 = yes)
stroke_flg: Stroke (categorical: 0 = no, 1 = yes)
mal_flg: Malignancy (categorical: 0 = no, 1 = yes)
resp_flg: Respiratory disease (non-COPD) (categorical: 0 = no, 1 = yes)
map_1st: Mean arterial pressure (mmHg, quantitative)
hr_1st: Heart Rate (quantitative)
temp_1st: Temperature (F, quantitative)
spo2_1st: S_pO_2 (%, quantitative)
abg_count: arterial blood gas count (number of tests, quantitative)
wbc_first: first White blood cell count (K/uL, quantitative)
hgb_first: first Hemoglobin (g/dL, quantitative)
platelet_first: first Platelets (K/u, quantitativeL)
sodium_first: first Sodium (mEq/L, quantitative)
potassium_first: first Potassium (mEq/L, quantitative)
tco2_first: first Bicarbonate (mEq/L, quantitative)
chloride_first: first Chloride (mEq/L, quantitative)
bun_first: first Blood urea nitrogen (mg/dL, quantitative)
creatinine_first: first Creatinine (mg/dL, quantitative)
po2_first: first PaO_2 (mmHg, quantitative)
pco2_first: first PaCO_2 (mmHg, quantitative)
iv_day_1: input fluids by IV on day 1 (mL, quantitative)




**III. Results **

```{r importing libs}
library(tidyverse)
library(randomForest)
library(iml)
library(FNN)
library(e1071) #support vector regression
library(pROC)
library(quantreg) #quant reg
library(vtable)
library(ISLR)
library(class)
library(sjlabelled)
library(corrplot)
library(cowplot)
```


```{r reading in dataset}
data <- read.csv("full_cohort_data.csv")
```

##Data Pre-processing:
To enhance the accuracy/reliability of our predictive models, we convert the categorical vars to factors, so as not to be confused with our numeric variables of interest.

```{r convert to factors}
data <- data %>%
  mutate(gender_num = factor(gender_num,
                             levels = c(0,1),
                             labels = c("Female", "Male")),
         aline_flg = factor(aline_flg,
                             levels = c(0,1),
                             labels = c("Year", "No")),
         service_num = factor(service_num,
                              levels = c(0,1),
                              labels = c("MICU or FICU", "SICU")),
         icu_exp_flg = factor(icu_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         hosp_exp_flg = factor(hosp_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         day_28_flg = factor(day_28_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         censor_flg = factor(censor_flg,
                               levels = c(0,1),
                               labels = c("Death","Censored")),
         sepsis_flg = factor(sepsis_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         chf_flg = factor(chf_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         afib_flg = factor(afib_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         renal_flg = factor(renal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         liver_flg = factor(liver_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         copd_flg = factor(copd_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         cad_flg = factor(cad_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         stroke_flg = factor(stroke_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         mal_flg = factor(mal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         resp_flg = factor(resp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes"))
         )
```


```{r imputing NA values}
na_indicies <- which(is.na(data), arr.ind=TRUE)  
# Function to calculate the mode
Mode <- function(x) {
  xx <- unique(x)
  xx[which.max(tabulate(match(x, xx)))]
}
#replace the factor variables with the mode
for(i in 1:ncol(data)){
  for(j in 1:nrow(data)){
    if(class(data[[i]])== "numeric"){
      if (is.na(data[j,i])){
        data[j,i] <- median(as.numeric(unlist(data[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(data[j,i])){
        data[j,i]<-Mode(na.omit(data[,i]))
    }
  }
  }
}


```
We have ~1200 total vars that are NA, so we begin by replacing categorical NA with the mode of the column, and numeric NA's with the median. We then create a function that takes in a proximity matrix and updates imputed NA values 

#Mean / Mode table
```{r getting the mean and the mode of all columns for summary purposes}
meanModeTable <- data
#replacing all observations  in  a column  with the median or mode
for(i in 1:ncol(meanModeTable)){
  if(class(meanModeTable[,i]) == "numeric"){
    meanModeTable[,i] <- mean(meanModeTable[,i])
  }else{
    meanModeTable[,i] <- Mode(na.omit(meanModeTable[,i]))
  }
}
#slicing out only one row (all we need)
meanModeTable <- meanModeTable[1,]

```

```{r function to update NA values based on the prox matrix}
iterate <- function(proxMatrix){
  for (i in 1:nrow(na_indicies)){
  column_names <-colnames(data)
  # Storing indicies of the na value
  rowNum <- na_indicies[i,1]
  colNum <- na_indicies[i,-1]
  column_name <- column_names[colNum]
  # If the column contains numeric variables as opposed to categorical
  if(sapply(data[,colNum], class)[1] == "numeric"){
    # print("Entering col 5")
    #updtating NA val with imputed val across the column
    imputed_val <- sum(data[-rowNum,column_name] * proxMatrix[-rowNum,rowNum]/ sum(proxMatrix[-rowNum,rowNum]))
    data[rowNum,colNum] <- imputed_val
    
  }else{ #Contains categorical vars, not numeric
    # print("entering other col")
    #df to store proximity excluding the row of intrest
    df_factor <- data.frame(proximity = proxMatrix[-rowNum,rowNum],
                            class = data[-rowNum,column_name])
    
    val <- df_factor %>% #computing for mode
      group_by_at("class") %>%
      summarize(avg = mean(proximity))
    index <- which.max(val$avg)
    data[rowNum,column_name] <- val[index,1]
  }
}
  
  
}
```





Run the function above 10 times on our data
```{r imputing na data}
#only doing 5 iterations due to the large number of NA's
for (i in 1:5){
  #Fitting random forest to predict censor + getting prox mtx
  rf <- randomForest(formula = censor_flg ~., 
                   data = data,
                   proximity = TRUE)
  pmtx <- data.frame(rf$proximity)
  
  iterate(pmtx)
  print(paste("iteration", i , "done"))
}

#creates dataframe "data"- base dataframe for the rest of the project
```





**QUESTION OF INTEREST #1: PREDICTING LENGTH OF ICU STAY**


Due to the ominous nature of ICU patients, no singular model should be used in all situations. We therefore employ multiple models for robust predictions of ICU length of stay. We start with a linear regression model, as there is a clear interpretation of the relationship between our variables of interest and ICU length of stay outcomes. We then compare outcomes of linear regression to support vector regression, which provides a better fit on non-linear data. We conclude with a random forest model which utilizes cross validation, and will not be subjected to over fitting.

#LINEAR REGRESSION FOR ICU STAY

For predicting the length of stay in the icu
  
```{r mean, median for ICU stay}
mean(data$icu_los_day)
median(data$icu_los_day)
```
Looking at some summary information, the average length of stay in the ICU was around 3.3 days, and the median was around 2.2 days. One can see that the mean is much higher than the median, indicating the data is skewed to the right. 

Continuing with some summary info, plotting brief exploratory quantile regression for age and ICU stay
```{r brief quantile regression for observation of age on icu stay}
colors <- c("#ffe6e6", "#ffcccc", "#ff9999", "#ff6666", "#ff3333",
            "#ff0000", "#cc0000", "#b30000", "#800000", "#4d0000", "#000000") #colors for regression lines
multi_rqfit <- rq(icu_los_day ~ age, data = data, tau = seq(0, 1, by = 0.1)) #quantile regrssion
plot(icu_los_day ~ age, data = data, pch = 16) #plotting reg
for (j in 1:ncol(multi_rqfit$coefficients)) { #plotting quartiles
    abline(coef(multi_rqfit)[, j], col = colors[j])
}
```
obviously, as the age of the patient admitted goes up, the average predicted ICU stay will also increase. This is apparent across all quantiles with all linear coefficients.

```{r mean, median, and mode for the length of ICU stay}
ggplot(data = data,
       aes(x = icu_los_day))+
  geom_histogram(binwidth = 0.5) +
  labs(x = "Length of Stay in the ICU (Days)")+
  theme_bw()
```
Looking at the distribution of length of stays, you can see that there are many what people would consider outliers in the graph, there are numerous stays beyond 10 days

```{r maybe facet wrap based on a variable of intrest?}
ggplot(data = data,
       aes(x = icu_los_day))+
  geom_histogram(binwidth = 0.4) +
  labs(x = "Length of Stay in the Hospital (Days)") +
  facet_grid(~service_num)+
  theme_bw()
```
When checking medical intensive care unit vs surgical intensive care unit, both seem to have similar distributions and a non-zero number of outliers. Therefore, this length of stay is not unique to the type of ICU that the patient was admitted into.

```{r boxplot for outliers}
#computing outlines
outlier_data <- data %>%
  mutate(outlier = icu_los_day > median(icu_los_day) + IQR(icu_los_day) * 1.5)
ggplot(data = outlier_data,
       aes(x = icu_los_day,
           y =  ""))+
  geom_boxplot() +
  labs(x = "Length of Stay in the ICU (Days)")+
  theme_bw()
```
when looking at this data, we can see we have a large distribution of stays (1776) around the 2-4 day mark, but there are also 214 outliers in the data. These two are equally important to distinguish, as long stays in the ICU are resource intensive and important for allocation with finite spaces.

```{r getting the percent of outliers}
count(outlier_data %>% filter(outlier == TRUE))
count(outlier_data %>% filter(outlier == TRUE)) / count(outlier_data %>% filter(outlier == FALSE)) *100
```
These calculations show that 13.7 percent of our patients admitted into the ICU were outliers (214 patients in our dataset)

#FIT LR models on non-outliers, or short stay, vs outliers, or long stay
Because of the outliers above, we decided to split training data into outliers vs non outliers, then fit two linear models for short stay and long stay.When predicting for our data, we would then flow down our observation, first classfying if the data classified it as an outlier, then fitting the consequential linear model on that data. We found that this was a crude way to greatly improve the performance of a linear model on our dataset.

#For short stay/long
```{r splitting into testing/training short stay}
set.seed(1)
short_stay <- outlier_data %>% filter(outlier == FALSE)
long_stay <- outlier_data %>% filter(outlier == TRUE)
n.short <- nrow(short_stay)
# Common to use 70% or 80% of observations for training set
train.indices.short <- sample(1:n.short, size = floor(0.8*n.short)) 
train_short <- short_stay %>%
  slice(train.indices.short)
test_short <- short_stay %>%
  slice(-train.indices.short)


n.long <- nrow(long_stay)
# 80% of observations for training set
train.indices.long <- sample(1:n.long, size = floor(0.8*n.long)) 
train_long <- long_stay %>%
  slice(train.indices.long)
test_long <- long_stay %>%
  slice(-train.indices.long)
```

```{r checking unique vars}
#count unique vars, or columns that all contain the same variable and therefore will be excluded from our regression model
sapply(lapply(data, unique), length)
```
Checking for unique variables, we can see sepsis_flg only has one variable, so we do not include it in our model fitting

```{r Linear regression model for shorter stay (on training)}
#Taking a linear model without sepsis, because that column has no unique values and will result in error if run
m.short.stay <- lm(formula = icu_los_day ~ ., data = subset(train_short,
                                                 select = -sepsis_flg)) 
#making predictions on the test data
pred_short_stay <- predict(m.short.stay, newdata = test_short)
#compute MSE and RMSE
mean( (test_short$icu_los_day - pred_short_stay)^2 ) # test MSE
sqrt( mean( (test_short$icu_los_day - pred_short_stay)^2 ) ) # test RMSE
```
How do these values compare to our longer stay model?

```{r}
summary(m.short.stay)
```
Statistically significant variables include: adg_count, age, and the service unit. 


#Repeating the steps for longer stay

```{r longer stay}
#same procedure as fitting our linear model for short stay
m.long.stay <- lm(formula = icu_los_day ~ ., data = subset(train_long,
                                                 select = -sepsis_flg)) 
#making predictions on the test data
pred_long_stay <- predict(m.long.stay, newdata = test_long)
#compute MSE and RMSE
mean( (test_long$icu_los_day - pred_long_stay)^2 ) # test MSE
sqrt( mean( (test_long$icu_los_day - pred_long_stay)^2 ) ) # test RMSE
```
comparatively, these are two small error rates. For a test observation, we will therefore calculate if it is an outlier as we did above, and filter that result down into an appropriate linear model of prediction.

```{r}
summary(m.short.stay)
```
Statistically significant variables are the same as short stay. This means that for the most part, there are no differentiating variables that contribute to identifying key variables for short stay vs long stay


How do these rates compare to if we were to fit only 1 model on the dataset?
#Testing on a singluar model for RMSE value comparison
```{r testing without outlier filtering (both short and long stays)}
set.seed(1)
n.los <- nrow(data)
# Common to use 70% or 80% of observations for training set
train_indices3 <- sample(1:n.los, size = floor(0.8*n.los)) 
train_compare <- data %>%
  slice(train_indices3)
test_compare <- data %>%
  slice(-train_indices3)
```

```{r testing without outlier filtering}
m.los <- lm(formula = icu_los_day ~ ., data = subset(test_compare,
                                                 select = -sepsis_flg))
pred_all_stay <- predict(m.los, newdata = test_compare)
#computing
mean( (test_compare$icu_los_day - pred_long_stay)^2 ) # test MSE
sqrt( mean( (test_compare$icu_los_day - pred_long_stay)^2 ) ) # test RMSE
```
Without splitting the observations into long stay vs short stay, our linear model does not work nearly as well as our two smaller models


#Comparing linear regression models to a support vector regression model

Linear regression typically cannot capture the non linearity in a dataset and therefore support vector regression machines can become handy in such situations. We compare both of our linear models (short and long stay) to support vector regression models in the same situation
```{r testing svm on short stay}
#fitting svm
modelsvm <- svm(icu_los_day ~.,
                data = subset(train_short, select = -sepsis_flg))
#predicting
predYsvm <- predict(modelsvm, newdata = test_short)
mean( (test_short$icu_los_day - predYsvm)^2 ) # test MSE
sqrt( mean( (test_short$icu_los_day - predYsvm)^2 ) ) # test RMSE
```
The mse and rmse for the support vector regression model on short stay were lower than the linear regression model on short stay. This is likely due to the fact that linear models assume linear relationships with covariates, and support vector machines can assist on providing a better fit on on linear data.

We now compare for long stay

```{r testing svm on long stay}
# #fitting svm
# modelsvm <- svm(icu_los_day ~.,
#                 data = subset(train_long, select = -sepsis_flg))
# #predicting
# predYsvm = predict(modelsvm, newdata = subset(test_long, select = -sepsis_flg))
# 
# mean( (test_long$icu_los_day - predYsvm)^2 ) # test MSE
# sqrt( mean( (test_long$icu_los_day - predYsvm)^2 ) ) # test RMSE
```
For some reason, the lack of observations in the long stay test set are resulting in errors. We can still fit another svm on both datasets together


```{r testing svm on stay}
#fitting svm
modelsvm <- svm(icu_los_day ~.,
                data = subset(train_compare, select = -sepsis_flg))
#predicting
predYsvm = predict(modelsvm, newdata = test_compare)
mean( (test_compare$icu_los_day - predYsvm)^2 ) # test MSE
sqrt( mean( (test_compare$icu_los_day - predYsvm)^2 ) ) # test RMSE
```
Across all predictions, support vector regression does better than linear regression on our dataset, reflecting the multi-dimensional nature of ICU patients



**SUMMARY RMSE for LM and SVM:**

Linear model for whole dataset: 
3.417015

linear model for short stay / long stay:
1.095784 / 1.020549

support vector model for whole dataset : 
1.925464

support vector model for short stay / long stay: 
0.9101927 / NA

Random forest for whole dataset (computed below):
2.153485

We can see the advantages of fitting support vector machines over linear regression models clearly displayed through the rmse.


#FITTING RANDOM FOREST for ICU STAY

How does a random forest compare to our linear model/ support vector model? Random forests additionally allow for exploration into important variables
```{r splitting testing/training again}
set.seed(1)
n.rf.los <- nrow(data)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n.rf.los, size = floor(0.8*n.rf.los)) 
train.rf.los <- data %>%
  slice(train_indices)
test.rf.los <- data %>%
  slice(-train_indices)
```

Filtering out variable with obvious covariance (length of stay in hospital)
```{r fitting random forest without the length of stay in the hospital}
updated.train.rf <- train.rf.los %>%
  dplyr::select(-hospital_los_day) #taking out length of stay in hospital???
#removing var with obvious covariance
rf.with.hospital <- randomForest(formula = icu_los_day ~., 
                   data = train.rf.los,
                   importance = TRUE)
#This is the one we're currently using , DOES NOT contain hospital_los_day
rf.los <- randomForest(formula = icu_los_day ~., 
                   data = updated.train.rf,
                   importance = TRUE)
```
Here, we filter out the length of stay in the hospital, because the length of stay in the ICU directly interacts with the length of stay in the hospital.


```{r predicting w test data and fitted forest}
pred.rf.los <- predict(rf.los, newdata = test.rf.los)
```

```{r computing accuracy compared to our split linear models}
mean( (test.rf.los$icu_los_day - pred.rf.los)^2 ) # test MSE
sqrt( mean( (test.rf.los$icu_los_day - pred.rf.los)^2 ) ) # test RMSE
```
our rmse was a higher for the random forest, but we also didn't split our data apart into outliers. This is a more hollistic model where we may not be able to distinguish a singular patient as an outlier, as there may be no testing data to compare quartiles with. 

```{r variable importance plot}
varImpPlot(rf.los, cex = 0.7)
```
arterial blood gas count was the strongest predictor for predicting length of ICU stay. An arterial blood gas analysis (ABG) measures the balance of oxygen and carbon dioxide in your blood to see how well your lungs are working. Naturally, if the lungs are not working well, the need to stay in the ICU will be longer.

You can additionally see from the distribution on the bottom that there may be one or two outliers skewing data, but as the arterial blood gas count increases we can predict the length of stay in the icu will be longer.

```{r partial dependence plot}
pred_obj <- Predictor$new(rf.los, data = data)
temp <- FeatureEffect$new(pred_obj, feature = "abg_count", method = "pdp")
plot(temp)
```
The length of stay increases significantly as the abg count increases. For the vast majority of ICU patients, their abg count is less than 15, meaning the influcence on ICU stay is relatively low. However, when the abg count exceeds this, predicted length of ICU stay climbs steeply, as between 15-30 the length of stay jumps 2 days.


```{r}
pred_obj <- Predictor$new(rf.los, data = data)
temp <- FeatureEffect$new(pred_obj, feature = "resp_flg", method = "pdp")
plot(temp)
```
Many other variables additionally had impacts on decision. Though less severe than abg count, resp_flg (respiratory infection) (among other variables) also resulted in a small increase in the predicted length of stay.


**QUESTION OF INTEREST #2: PREDICTING LIVER DISEASE**

For this research question we chose liver disease as the malady we hoped to predicting. We chose this disease because of the difficulty in diagnosing it without additional, and sometimes invasive, tests.

```{r}
table(data$liver_flg)
99/1677 *100
```
5.90% of patients in the data set had liver disease. This means we have few instances of liver disease to train our models on and test them on. This also means our specificity could be 94.1% if we did nothing more than guess "no" for every patient. All model results will need to take this fact into context.

```{r select data to predict patients with liver disease}
data_liver_disease <- data %>%
  select(age, weight_first, bmi, map_1st, hr_1st, temp_1st, spo2_1st
         , wbc_first, hgb_first,
         platelet_first, 
         sodium_first,
         potassium_first,
         tco2_first,
         chloride_first,
         bun_first,
         creatinine_first,
         po2_first,
         pco2_first, 
         liver_flg)
```

Our response variable is liver_flg, which is a factor of two levels (“No” and “Yes”). Observations with “Yes” are patients that have liver disease. 

Our predictor variables were only measurements that could easily be gathered by doctors in an ICU or information that would be gathered as a matter of routine. The variables selected can be seen in the code chunk above. Their definitions can been seen in our formal definitions section. 

With these measurement, we go about attempting to predict liver disease with information that is easily accessible for ICU doctors and nurses. 

#liver disease method one: logistic regression

Our first method for predicting liver disease is logistic regression. We picked logistic regression becauase of the yes/no nature of our response variable. A patient either has liver disease or does not have liver disease. Adtionally, as one of the first methods of prediction we were introduced too, and a simpler method of prediction as well, we feel comfortable with this method of regression. Thus, we wanted to see how it preformed on real world data, and see how it compares to the random forest method we will use later. 

first we split our data set into a test a training data set.

```{r split into test and train for liver disease}
p <- nrow(data_liver_disease)
set.seed(1)
# Common to use 70% or 80% of observations for training set
train_liver_indices <- sample(1:p, size = floor(0.7*p)) 
train_liver_data <- data_liver_disease %>%
  slice(train_liver_indices)
test_liver_data <- data_liver_disease %>%
  slice(-train_liver_indices)
```

Next, we fit a model using training data and get the summary to look at significant variables.  
```{r}
liver.glm <- glm(formula = liver_flg ~., data = train_liver_data, family = binomial)
summary(liver.glm)
```

We can see several significant variables including platlet_first, hr_1st, pco2_first, hgb_first, and wbc_first. 

Now, we make predictions using our test set on the model created with the training data. 

```{r}
pred_prob_liver <- predict(liver.glm, newdata = test_liver_data, type = "response")
```

Now, we are creating a loop to identify the best decision threshold, iterating over decision thresholds from 0.1 to 0.9 in intervals of 9. We will be caluclating accuracy, sensativity, and specificity for each. 
```{r use a loop to determine best decision threshold/ hyperparameter}
store_accuracy <- c()
store_sensitivity <- c()
store_specificity <- c()
for( t in seq(from = 0.1, to = 0.9, by = 0.1)) {
  pred_class_labels_liver <- ifelse(pred_prob_liver > t, "Yes", "No")
  conf_mtx_liver <- table(pred_class_labels_liver, test_liver_data$liver_flg)
  
  #accuracy
  store_accuracy[t*10] <- sum(diag(conf_mtx_liver))/sum(conf_mtx_liver)
  #sensitivity (true liver disease haves labeled as liver disease haves)
  store_sensitivity[t*10] <- conf_mtx_liver[2,2]/sum(conf_mtx_liver[,2])
  #specificity(people that do not have liver disease that were predicted as not having liver    disease)
  store_specificity[t*10] <- conf_mtx_liver[1,1]/sum(conf_mtx_liver[,1])
  
  
}
tibble(decision_threshold = seq(from = 0.1, to = 0.9, by = 0.1),
       accuracy = store_accuracy,
       sensitivity = store_sensitivity,
       specificity = store_specificity)
```

Im selecting a decision threshold of 0.4. This is getting a high sensitivity without sacrificing much in the way of accuracy or specificity. 

Next we create a confusion matrix. 

```{r}
pred_class_labels_liver <- ifelse(pred_prob_liver > 0.4, "Yes", "No") # convert to labels
conf_mtx_liver <- table(pred_class_labels_liver, test_liver_data$liver_flg)
conf_mtx_liver
```

The model seems good at predicting patients without liver disease and bad a predicting patients with liver disease. This is worrying given that a doctor wants to see higher sensativity in these cases.  

```{r sensitivity/specificity}
#accuracy
sum(diag(conf_mtx_liver))/sum(conf_mtx_liver)
#sensitivity (true liver disease haves labeled as liver disease haves)
conf_mtx_liver[2,2]/sum(conf_mtx_liver[,2])
#specificity(people that do not have liver disease that were predicted as not having liver disease)
conf_mtx_liver[1,1]/sum(conf_mtx_liver[,1])
```

These results are disappointing. Our accuracy and specificity are both fantastic at 94.7% and 98.4% respectively. However, this is not really the data a doctor would care about. We would rather have high sensitivity and catch everyone with liver disease, even if at the expense of a few false positives. Additionally, we could have achieved a similar specificity by simply guessing "no" for every patient. Hopefully we get better results with the random forest model.  

Our method for picking  decision threshold was somewhat crude. This was partially a result of the optimal threshold we got (0.07) This felt too low, and we were not sure how valid selecting a threshold this low was. That said, we wanted to include an analysis of how our model preforms with that threshold. That discussion occurs below.

```{r ROC curve}
roc_obj_liver <- roc(predictor = pred_prob_liver, 
               response = test_liver_data$liver_flg)
plot(roc_obj_liver, legacy.axes = TRUE)
```


```{r optimal threshold}
auc(roc_obj_liver)
coords(roc_obj_liver) %>%
  mutate(youden = sensitivity + specificity - 1) %>%
  slice_max(youden)
```

As a thought experiment we wanted to calculate the optimal threshold. We arrived at the absurdly low threshold of 0.07 as that threshold. With threshold, our model preforms much better, with  sensitivity of 70%. This comes at the expense of specificity, which is now only 85%. That said, we are happy to make this trade, as we care more about sensitivity, and catching postie cases. We are a little put off by how low the threshold is, and frankly, we are uncertin how valid setting a threshold that low is. This could just be another sign of model weakness; we need to set a threshold that low to capture positives. That said, it could also be a sign of how difficult liver disease is to catch, and it takes very slight aberations from normal to catch positive cases with the health metrics we are using.

#random forest to see importance on liver disease

Our second selected method for this research question was using a random forest to predict liver disease. We choose this for a few reasons. First, our data set is large, and random forrest excels at handling large data sets. Additionally, we were hoping we would get access to the much larger MIMIC IV data set, where random forest’s large data set capabilities would be very advantageous. Additionally, we are using several predictors, and there is not an obvious relationship between the predictors, the predictors when combined, and the response variable. Thus, we are dealing with potentially complex relationships that random forest is well suited for. Random forrest is a “black box” meaning it can be less interpretable than other predictive methods. That said, given the somewhat simplistic goal of this section, we are willing to sacrifice interpretability for results. Similarly, while computationally intensive, we are willing to sacrifice speed for results. 

We hope that this more "advanced" method of mking predictions will improve upon our logistic regression model. 
`
```{r random forest to determine predictor importance on liver disease}
rf_liver <- randomForest(formula =  liver_flg~ ., #for importance of liver disease
                   data = data_liver_disease,
                   importance = TRUE)
```

In this code chunk we fit a random forest model in order to view the variable importance plot to identify important variables. We can see in the code chunk below that some variables such as platelet_first, wbc_first, and age have more importance in identifying liver disease than, for example, spo2_1st or temp_1st. 

```{r variable importance plot.}
varImpPlot(rf_liver, cex = 0.7)
```

Next, we look towards actually predicting liver disease using the random forest model. First we set the seed and fit a model with our training data. 
```{r create random forest model to make predictions}
set.seed(6)
rf_prediction_liver <- randomForest(formula =  liver_flg~ ., #for importance of liver disease
                   data = train_liver_data,
                   importance = TRUE)
```

We then make predictions, using our random forest model, on our test data. 
```{r make predictions}
pred_apps <- predict(rf_prediction_liver, newdata = test_liver_data)
```

We then create a confusion matrix by combining our predicted and actual liver data for the test set. 
```{r create confusion matrix}
rf_liver_tests <- tibble(predicted = pred_apps,
       actual = test_liver_data$liver_flg)
```

```{r assess model performance}
cf <- table(rf_liver_tests)
#accuracy
sum(diag(cf))/sum(cf)
#sensitivity (true liver disease haves labeled as liver disease haves)
cf[2,2]/sum(cf[,2])
#specificity(people that do not have liver disease that were predicted as not having liver disease)
cf[1,1]/sum(cf[,1])
```

The results of the confusion matrix are disappointing. Our model performs worse than the simple logistic regression model. The accuracy is lower along with the sensitivity, which is arguably the single most important statistic for this research question. This is because physicians want to capture as many cases of liver disease as possible, even at the expense of false positives. An extra test may cost time and money, but a missed positive case may cost lives. The specificity suffers from the same issue that plagued the logistic regression model. It is impressive at 99.8%, but this is less impressive when we recognize that we could have guessed no for every observation and achieved a similar specificity. Thus, it is hard to say whether we have an impressive model or just one that is wrong in the right way. 



**Question 3: Predicting Mortality**

To predict mortality, we first test and compare the accuracy of models built using KNN-Classification, Random Forest, and Logistic Regression on the entirety of the data set. We chose censor_flg(, a binary variable where 1 indicates censoring, or survival, of the patient and 0 indicates death within 730 days (2 years), as our target predictor. 

We consider the observations of patients for which our models predicted mortality to be “high-risk,” and observations for patients for which our model predicted survival to be “low-risk.”  As such, sensitivity, which is generally the accuracy in predicting mortality, can be interpreted as the percentage of high-risk patients who passed away, which we describe as the observed mortality for the high risk group.  Similarly, specificity can be interpreted as the percentage of low-risk patients who are deceased, which we describe as the observed mortality for the low-risk group.  To assess the overall performance of our models, we will compare their accuracy, sensitivity, and specificity, both amongst each other and against the literature.

```{r}
set.seed(1)
data.mortality <- read.csv("full_cohort_data.csv")
```
This section requires removing some variables that were held in the data set earlier.  Therefore we reload the data and reimpute similar to the process above



```{r convert to factors again}
data.mortality <- data.mortality %>%
  mutate(gender_num = factor(gender_num,
                             levels = c(0,1),
                             labels = c("Female", "Male")),
         aline_flg = factor(aline_flg,
                             levels = c(0,1),
                             labels = c("Year", "No")),
         service_num = factor(service_num,
                              levels = c(0,1),
                              labels = c("MICU or FICU", "SICU")),
         icu_exp_flg = factor(icu_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         hosp_exp_flg = factor(hosp_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         day_28_flg = factor(day_28_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         censor_flg = factor(censor_flg,
                               levels = c(0,1),
                               labels = c("Death","Censored")),
         sepsis_flg = factor(sepsis_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         chf_flg = factor(chf_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         afib_flg = factor(afib_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         renal_flg = factor(renal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         liver_flg = factor(liver_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         copd_flg = factor(copd_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         cad_flg = factor(cad_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         stroke_flg = factor(stroke_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         mal_flg = factor(mal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         resp_flg = factor(resp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes"))
         )
```


```{r}
data.mortality<- data.mortality %>%
  select(-day_icu_intime, -mort_day_censored, -day_28_flg, -hosp_exp_flg, -icu_exp_flg, -hospital_los_day)
```


```{r imputing NA values}
na_indicies <- which(is.na(data.mortality), arr.ind=TRUE)  

#replace the factor variables with the mode
for(i in 1:ncol(data.mortality)){
  for(j in 1:nrow(data.mortality)){
    if(class(data.mortality[[i]])== "numeric"){
      if (is.na(data.mortality[j,i])){
        data.mortality[j,i] <- median(as.numeric(unlist(data.mortality[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(data.mortality[j,i])){
        data.mortality[j,i]<-Mode(na.omit(data.mortality[,i]))
    }
  }
  }
}
```



```{r}
iterate.mortality <- function(proxMatrix){
  for (i in 1:nrow(na_indicies)){
  column_names <-colnames(data.mortality)
  # Storing indicies of the na value
  rowNum <- na_indicies[i,1]
  colNum <- na_indicies[i,-1]
  column_name <- column_names[colNum]
  # If the column contains numeric variables as opposed to categorical
  if(sapply(data.mortality[,colNum], class)[1] == "numeric"){
    # print("Entering col 5")
    #updtating NA val with imputed val across the column
    imputed_val <- sum(data.mortality[-rowNum,column_name] * proxMatrix[-rowNum,rowNum]/ sum(proxMatrix[-rowNum,rowNum]))
    data.mortality[rowNum,colNum] <- imputed_val
    
  }else{ #Contains categorical vars, not numeric
    # print("entering other col")
    #df to store proximity excluding the row of intrest
    df_factor <- data.frame(proximity = proxMatrix[-rowNum,rowNum],
                            class = data.mortality[-rowNum,column_name])
    
    val <- df_factor %>% #computing for mode
      group_by_at("class") %>%
      summarize(avg = mean(proximity))
    index <- which.max(val$avg)
    data.mortality[rowNum,column_name] <- val[index,1]
  }
}
  
  
}
```

```{r imputing na data}
for (i in 1:5){
  #Fitting random forest to predict censor + getting prox mtx
  rf <- randomForest(formula = censor_flg ~., 
                   data = data.mortality,
                   proximity = TRUE)
  pmtx <- data.frame(rf$proximity)
  
  iterate.mortality(pmtx)
  print(paste("iteration", i , "done"))
}
```

```{r}
importance(rf)
```

```{r}
#variable importance plot
varImpPlot(rf, cex = 0.5)
```

It appears cp and ca are important under both MDI and MDA

```{r}
age_obj <- Predictor$new(rf, data = data.mortality)
age_plot <- FeatureEffect$new(age_obj, feature = "age", method = "pdp", center.at = 0)
plot(age_plot)
```


```{r}
bun_obj <- Predictor$new(rf, data = data.mortality)
bun_plot <- FeatureEffect$new(bun_obj, feature = "bun_first", method = "pdp", center.at = 0)
plot(bun_plot)
```

```{r}
los_obj <- Predictor$new(rf, data = data.mortality)
bun_plot <- FeatureEffect$new(los_obj, feature = "sapsi_first", method = "pdp", center.at = 0)
plot(bun_plot)
```

### Predict Using Random Forest ####
### Predict With Random Forest

Predicting mortality with a random forest, we split the data into a training and test set, with 70% of data in the training set.  We then fit a random forest model on the training set, which is then used to make predictions on the testing set.  We compute accuracy, sensitivity, and specificity to assess our model’s performance.

```{r}
rf.data<- data.mortality
```


```{r splitting testing/training again}
set.seed(1)
n.rf <- nrow(rf.data)
# Common to use 70% or 80% of observations for training set
train.indices.rf <- sample(1:n.rf, size = floor(0.7*n.rf)) 
train.rf <- rf.data %>%
  slice(train.indices.rf)
test.rf <- rf.data %>%
  slice(-train.indices.rf)
```


```{r fit rf to predict mortality}
 #taking out length of stay in hospital???
#removing var with obvious covariance
rf.mortality <- randomForest(formula = censor_flg ~., 
                   data = train.rf,
                   importance = TRUE)

```



```{r predicting w test data and fitted forest}
pred.rf.mortality <- predict(rf.mortality, newdata = test.rf, type = "response")

```



```{r}
#Use the ideal decision threshold from the Youden Index calculation
#pred.labels.rf <- ifelse(pred.rf.mortality > 0.55057, "Censored", "Death") # use the ideal threshold
#make a confusion matrix
conf_mtx <- table(pred.rf.mortality, test.rf$censor_flg)
conf_mtx

```

```{r}
rf.accuracy = sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
rf.specificity = conf_mtx[2,2]/sum(conf_mtx[,2]) # specificity
rf.sensitivity = conf_mtx[1,1]/sum(conf_mtx[,1]) # sensitivity

rf.accuracy
rf.specificity
rf.sensitivity
```

The random forest model performed with the same overall accuracy as the logistic regression model (below), but the random forest outperformed the logistic regression model in the low risk group, with only 9.9% incidence of mortality within 2 years,  but performed slightly than logistic regression in predicting the high-risk group, observing 55% 2 year mortality


```{r}
varImpPlot(rf.mortality, cex = 0.7)
```
We find that age, stroke, malignant cancer presence, sapsi score, and blood urea nitrogen levels are most important to the accuracy of the random forest model, while blood urea nitrogen, sapsi score, and white blood cell count are most important for the homogeneity of the nodes and leaves.

It is notable that MeanDecreaseAccuracy tends to hold factor variables such as incidence of stroke in higher regard, while MeanDecreaseGini tends to prioritize continuous variables

```{r}
pred_obj <- Predictor$new(rf.mortality, data = rf.data)
bun_plot <- FeatureEffect$new(pred_obj, feature = "age", method = "pdp", center.at = 0)
plot(bun_plot)
```

```{r}
pred_obj <- Predictor$new(rf.mortality, data = rf.data)
age_plot <- FeatureEffect$new(pred_obj, feature = "bun_first", method = "pdp", center.at = 0)
plot(age_plot)
```

```{r}
pred_obj <- Predictor$new(rf.mortality, data = rf.data)
age_plot <- FeatureEffect$new(pred_obj, feature = "sapsi_first", method = "pdp", center.at = 0)
plot(age_plot)
```


```{r}
pred_obj <- Predictor$new(rf.mortality, data = rf.data)
age_plot <- FeatureEffect$new(pred_obj, feature = "age", method = "pdp")
plot(age_plot)
```

#USING KNN Classification to Predict Mortality

For KNN-Classification, we filter out all factor variables, including gender and incidences of stroke, congestive heart failure, and chronic obstructive pulmonary disease, since this method of modeling is based on the “distance” between two observations and it is impossible to find the distance between factor observations.  We then split the data into train and test sets, and then performed 10-fold cross validation on the training set in order to obtain the optimal nearest neighbors, which was determined to be 12. We then fit a 12NN-Classification model on the entire training set, which was then used to make predictions on the test set. To assess the accuracy of our predictions, we create a confusion matrix to observe the overall accuracy, sensitivity, and specificity of the model.

```{r}
set.seed(1)
#data.mortality<- data
#check for which collumns are numeric
num_cols <- unlist(lapply(data.mortality, is.numeric))
```

```{r}
#keep only numeric continuous
data.knn <- data.mortality[ , num_cols] 
```


```{r}
#add censor_flg back in
data.knn$censor_flg <- data.mortality$censor_flg
```


## Split Into Training vs. Test

```{r split into training and test}
#split into training and test sets
set.seed(1)
n.knn <- nrow(data.knn)
train.indices.knn <- sample(1:n.knn, size = floor(0.8*n.knn))
train.knn <- data.knn %>% slice(train.indices.knn)
test.knn <- data.knn %>% slice(-train.indices.knn)
```

## k-Fold Cross-Validation

```{r assign observations to folds}
num_folds <- 10 # number of folds
# Create a variable (`fold_id`) assigning each observation in the training
# set to a fold between 1 and `num_folds`
train.knn <- train.knn %>%
  mutate(fold_id = 1:n() %% num_folds)
```

```{r k-fold CV}
# Vector of numbers of nearest neighbors 
K_seq <- 1:60
# Create vector to store k-fold cross-validation error for each number of
# nearest neighbors
store_kCV_err <- c()
# Loop over numbers of nearest neighbors
for(num_near in K_seq) {
  
  # Create vector to store model error for each held-out fold
  store_fold_err <- rep(0, num_folds)
  
  # Loop over folds
  # Remember that `fold_id` goes from 0 to (number of folds - 1)
  for(i in 0:(num_folds-1)) { 
    
    # Train on all but the ith fold
    # Separate into predictors (X) and response (Y)
    train_X <- train.knn %>% 
      filter(fold_id != i) %>% 
      select(-censor_flg, -fold_id) 
    train_Y <- train.knn %>% 
      filter(fold_id != i) %>% 
      pull(censor_flg) 
    
    # Test on the ith fold
    # Separate into predictors (X) and response (Y)
    test_X <- train.knn %>% 
      filter(fold_id == i) %>% 
      select(-censor_flg, -fold_id) 
    test_Y <- train.knn %>%
      filter(fold_id == i) %>%
      pull(censor_flg)
    
    # Normalize the predictors in the training and test sets, using the means and
    # standard deviations from the training set only
    train_means <- apply(train_X, MARGIN = 2, FUN = mean)
    train_sds <- apply(train_X, MARGIN = 2, FUN = sd)
    train_X_norm <- scale(train_X)
    test_X_norm <- scale(test_X, center = train_means, scale = train_sds)
    
    # Run KNN classification with the given number of nearest neighbors
    knn_preds <- knn(train = train_X_norm, 
                     test = test_X_norm, 
                     cl = train_Y, 
                     k = num_near)
    
    # Store the model error for this fold
    # The model error is the proportion of incorrectly classified
    # observations in the hold-out fold
    store_fold_err[i+1] <- sum(knn_preds != test_Y)/length(test_Y)
    
  } 
  
  # Store the k-fold CV error for this number of nearest neighbors
  # The k-fold CV error is the average model error across all the held-out
  # folds
  store_kCV_err <- c(store_kCV_err, mean(store_fold_err))
  
}
```

```{r plot k-fold CV error as function of K}
# Plot the k-fold CV error as a function of the number of nearest neighbors
ggplot(tibble(K = K_seq, err = store_kCV_err), aes(x = K, y = err)) +
  geom_point() +
  geom_line() +
  labs(title = "K-Fold Cross-Validation Error",
       subtitle = "KNN Classification",
       x = "Number of Nearest Neighbors",
       y = "Average Proportion of Misclassified Observations")
```

## Fit Final Model

```{r choose value of K}
# Choose optimal value of K based on error plot above
# Note that this is somewhat subjective!
opt_num_near <- 12
```

```{r fit final model}
# Fit the final model on the full training set, using the optimal K chosen
# above
# Separate full training set into predictors (X) and response (Y)
train_X <- train.knn %>% 
  select(-censor_flg, -fold_id) 
train_Y <- train.knn %>% 
  pull(censor_flg) 
# Separate test set into predictors (X) and response (Y)
test_X <- test.knn %>% 
  select(-censor_flg) 
test_Y <- test.knn %>%
  pull(censor_flg)
# Normalize the predictors in the training and test sets, using the means and
# standard deviations from the training set only
train_means <- apply(train_X, MARGIN = 2, FUN = mean)
train_sds <- apply(train_X, MARGIN = 2, FUN = sd)
train_X_norm <- scale(train_X)
test_X_norm <- scale(test_X, center = train_means, scale = train_sds)
knn_preds_final <- knn(train = train_X_norm, 
                       test = test_X_norm, 
                       cl = train_Y, 
                       k = opt_num_near) # use optimal K
```

```{r assess final model performance}
# Make confusion matrix
conf_mtx <- table(knn_preds_final, test_Y)
conf_mtx
sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # sensitivity
conf_mtx[1,1]/sum(conf_mtx[,1]) # specificity
```

In the KNN Classification model (Table 2), overall accuracy was 78.4%, with observed 2 year mortality in the high-risk group of 50.5% and observed mortality in the low risk group of 11.2%



#### LOGISTIC REGRESSION TO PREDICT MORTALITY

For Logistic Regression, we omitted studying sepsis (sepsis_flg) from the model, as no patients in the data set contracted sepsis (see discussion). We then split the data into train and test sets, with 70% of data in the training set and 30% in the test set. We fit a general linear model using logistic regression on the training set, which was then used to make predictions on the test set.  The predictions take the form of a vector of probabilities, between 0 and 1, based on the determined likelihood of the observation being predicted as Censored.  To determine which observations to classify as “Censored” and which to classify as “Death”, we graph a receiver operating characteristic (ROC) curve, which maps the sensitivity vs specificity of the model.  We use the ROC curve to determine the Youden index, or the ideal threshold to maximize sensitivity and specificity in the model. Observations above the Youden index are then classified as “Censored,” and below are classified as “Death.”  Again, we assess the accuracy of our predictions through a confusion matrix.

```{r}
#Split into training and test set
set.seed(1)
n.glm <- nrow(data.mortality)
train.indices.glm <- sample(1:n.glm, size = floor(0.7*n.glm))
train.glm <- data.mortality %>%slice(train.indices.glm)
test.glm <- data.mortality %>%slice(-train.indices.glm)
```

```{r imputing NA values for training set}
na_indicies_data_mortality <- which(is.na(data.mortality), arr.ind=TRUE)  
# Function to calculate the mode

#replace nas- factor with mode, numeric with mean
for(i in 1:ncol(train.glm)){
  for(j in 1:nrow(train.glm)){
    if(class(train.glm[[i]])== "numeric"){
      if (is.na(train.glm[j,i])){
        train.glm[j,i] <- mean(as.numeric(unlist(train.glm[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(train.glm[j,i])){
        train.glm[j,i]<-Mode(na.omit(train.glm[,i]))
    }
  }
  }
}
```

```{r imputing NA values for test set}
#na_indicies <- which(is.na(data), arr.ind=TRUE)  

#replace the factor variables with the mode
for(i in 1:ncol(test.glm)){
  for(j in 1:nrow(test.glm)){
    if(class(test.glm[[i]])== "numeric"){
      if (is.na(test.glm[j,i])){
        test.glm[j,i] <- mean(as.numeric(unlist(train.glm[,i])), #impute with trainig set means even for test set 
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(test.glm[j,i])){
        test.glm[j,i]<-Mode(na.omit(train.glm[,i]))
    }
  }
  }
}
```



Part 2: Fit logistic regression
```{r}
#fit logistic regression model
m.glm.mortality <- glm(formula = censor_flg ~ ., 
          data = subset(train.glm,select = -sepsis_flg), 
          family = binomial)
summary(m.glm.mortality)
```
In the logistic regression model, some notable statistically significant variables include age, gender, incidence of stroke, cancer, and chronic obstructive pulmonary disease, and blood urea nitrogen, albeit with a small coefficient. Holds factor variables in higher importance, as well.

Question 3: Model Evaluation
```{r}
#Calculate the probability of fault in the test set
pred.glm.mortality <- predict(m.glm.mortality, newdata = test.glm, type = "response") # predicted probabilities
# roc() function to plot an ROC curve for the test set
roc_obj_glm <- roc(predictor = pred.glm.mortality, 
               response = test.glm$censor_flg)
plot(roc_obj_glm, legacy.axes = TRUE)
```

```{r}
auc(roc_obj_glm)
```
This is somewhat better than a model that randomly guesses.


```{r}
#find youden index and ideal decision threshold
coords(roc_obj_glm) %>%
  mutate(youden = sensitivity + specificity - 1) %>%
  slice_max(youden)
```
The ROC curve looks like a staircase because there are only 4 positives in the test set. Lowering the decision threshold slightly usually affects the specificity (by adding false positives) without changing the sensitivity (no additional true positives). In other words, when the model correctly classifies a positive, the sensitivity jumps. The height of the jumps are all the same because each one is a 25% increase in the true positive rate. The widths of the steps correspond to the number of false positives with a predicted probability between each true positive. This number of false positives is not consistent which is why the steps have differing widths.

```{r}
#Use the ideal decision threshold from the Youden Index calculation
pred_class_labels_glm_mortality <- ifelse(pred.glm.mortality > 0.5866, "Yes", "No") # use the ideal threshold
#make a confusion matrix
conf_mtx <- table(pred_class_labels_glm_mortality, test.glm$censor_flg)
conf_mtx
```

```{r}
sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # specificity
conf_mtx[1,1]/sum(conf_mtx[,1]) # sensitivity
```

In the logistic regression model, overall accuracy was 80.3%, with an observed 2 year mortality of 70.5% in the high risk group and 15.9% in the low risk group


### Split data in half for old vs young


In all models, however, the importance of age in predicting mortality is undeniable.  Due to the strong response of predicting mortality as a result of old age, we split the data into two age groups, “young” and “old,” based on the median age (56.3 years) in the data set, in order to make predictions more accurately in different age brackets.

we split the data set in half, with observations of patients above the median age placed in one data set, and observations of patients below the median age placed in a separate data set.  We then follow similar processes as above to fit models using logistic regression and random forests on each data set.  

```{r}
rf.old<- data.mortality %>%
  filter(age> median(data.mortality$age))
```


```{r splitting testing/training again}
set.seed(1)
n.old <- nrow(rf.old)
# Common to use 70% or 80% of observations for training set
train.indices.old <- sample(1:n.old, size = floor(0.7*n.old)) 
train.old <- rf.old %>%
  slice(train.indices.old)
test.old <- rf.old %>%
  slice(-train.indices.old)
```


```{r}
 #taking out length of stay in hospital???
#removing var with obvious covariance
glm.old <- glm(formula = censor_flg ~., 
                   data = subset(train.old, select = -sepsis_flg),
                   family = binomial)
```



```{r predicting w test data and fitted forest}
pred.glm.old<- predict(glm.old, newdata = test.old, type = "response")
```


```{r}
#Calculate the probability of fault in the test set
# roc() function to plot an ROC curve for the test set
roc_obj <- roc(predictor = pred.glm.old, 
               response = test.old$censor_flg)
plot(roc_obj, legacy.axes = TRUE)
```

```{r}
auc(roc_obj)
```
This is somewhat better than a model that randomly guesses.


```{r}
#find youden index and ideal decision threshold
coords(roc_obj) %>%
  mutate(youden = sensitivity + specificity - 1) %>%
  slice_max(youden)
```


```{r}
#Use the ideal decision threshold from the Youden Index calculation
pred_class_labels <- ifelse(pred.glm.old > 0.4749, "Yes", "No") # use the ideal threshold
#make a confusion matrix
conf_mtx <- table(pred_class_labels, test.old$censor_flg)
conf_mtx
```

```{r}
sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # specificity
conf_mtx[1,1]/sum(conf_mtx[,1]) # sensitivity
```


```{r fitting rf on old age}
rf.old.model <- randomForest(formula = censor_flg ~., 
                   data = subset(train.old, select = -sepsis_flg),
                   importance = TRUE)
```


```{r predicting w test data and fitted forest}
pred.rf.old <- predict(rf.old.model, newdata = test.old, type = "response")
```


```{r}
#Use the ideal decision threshold from the Youden Index calculation

#make a confusion matrix
conf_mtx <- table(pred.rf.old, test.old$censor_flg)
conf_mtx
```

```{r}
sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # specificity
conf_mtx[1,1]/sum(conf_mtx[,1]) # sensitivity
```

For both logistic and random forest models, we achieve moderately better mortality prediction for the old age group (mean age = 72.3, mortality rate = 46.3%, N=888) but suffer losses in overall accuracy and prediction of survival

### Predict on young
```{r}
#filter for data under median age
rf.young<- data.mortality%>%
  filter(age< median(data.mortality$age))
```

```{r splitting testing/training again}
set.seed(1)
n.young<- nrow(rf.young)
# Common to use 70% or 80% of observations for training set
train.ind.young <- sample(1:n.young, size = floor(0.7*n.young)) 
train.rf.young <- rf.young %>%
  slice(train.ind.young)
test.rf.young <- rf.young %>%
  slice(-train.ind.young)
```


```{r fit glm to predict mortality}
glm.young <- glm(formula = censor_flg ~., 
                   data = subset(train.rf.young, select = -sepsis_flg),
                   family = binomial)
```


```{r predicting w test data and fitted forest}
pred.prob.young.glm<- predict(glm.young, newdata = test.rf.young, type = "response")
```



```{r}
#Calculate the probability of fault in the test set
# roc() function to plot an ROC curve for the test set
roc_obj <- roc(predictor = pred.prob.young.glm, 
               response = test.rf.young$censor_flg)
plot(roc_obj, legacy.axes = TRUE)
```

```{r}
auc(roc_obj)
```



```{r}
#find youden index and ideal decision threshold
coords(roc_obj) %>%
  mutate(youden = sensitivity + specificity - 1) %>%
  slice_max(youden)
```


```{r}
#Use the ideal decision threshold from the Youden Index calculation
pred_class_labels <- ifelse(pred.prob.young.glm > 0.4235, "Yes", "No") # use the ideal threshold
#make a confusion matrix
conf_mtx <- table(pred_class_labels, test.rf.young$censor_flg)
conf_mtx
```

```{r}
sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # specificity
conf_mtx[1,1]/sum(conf_mtx[,1]) # sensitivity
```


```{r}
sensitivity<- c()
specificity<- c()
accuracy<- c()
for(i in seq(1,500,30)){
  rf.young.model <- randomForest(formula = censor_flg ~., 
                   data = subset(train.rf.young, select = -sepsis_flg),
                   importance = TRUE,
                   ntree = i)
  pred.young <- predict(rf.young.model, newdata = test.rf.young, type = "response")
  conf_mtx <- table(pred.young, test.rf.young$censor_flg)
  sensitivity <- append(sensitivity, conf_mtx[1,1]/sum(conf_mtx[,1]))
  specificity <- append(specificity, conf_mtx[2,2]/sum(conf_mtx[,2]))
  accuracy <- append(accuracy, sum( diag(conf_mtx) )/sum(conf_mtx))
}
```


```{r}
df2 <- data.frame(mortality_accuracy = sensitivity, 
                  censor_accuracy = specificity, 
                  accuracy = accuracy,
                  ntrees= seq(1, 500, 30))
```

```{r}
df2 %>%
  ggplot()+
  geom_line(aes(x =ntrees, y = accuracy, colour = "green"))+
  geom_line(aes(x = ntrees, y = censor_accuracy, colour = "blue"))+
  geom_line(aes(x = ntrees, y = mortality_accuracy, colour = "red"))
##ntrees = 181 maximizes complexity, and accuracy across all three levels
```



```{r 91 tree random forest}
set.seed(1)
rf.young.model <- randomForest(formula = censor_flg ~., 
                               data = subset(train.rf.young, select = (-sepsis_flg)),
                               importance = TRUE,
                               ntree = 181)
pred.young <- predict(rf.young.model, newdata = test.rf.young, type = "response")
conf_mtx <- table(pred.young, test.rf.young$censor_flg)
conf_mtx
conf_mtx[1,1]/sum(conf_mtx[,1])
conf_mtx[2,2]/sum(conf_mtx[,2])
sum( diag(conf_mtx) )/sum(conf_mtx)
```
In the young age group (mean age= 36.4, mortality rate = 9.7%, N = 888), we achieve higher accuracy and superior prediction of survival, but the observed 2 year mortality for the high risk group falls significantly, to 22.7% in the linear regression model and just 9.1% in the random forest model


All three models, regardless of data split, significantly outperformed random assignment to “Survival” and “Death” based on data means


### Comparing SAPSI Score
```{r}
sapsi.data<- data.mortality
```

```{r}
sapsi.data<- sapsi.data %>% mutate(logit = -7.7631+(0.0737*sapsi_first)+ (0.9971*log(sapsi_first+1)),
                                   predicted.mortality = exp(logit)/(1+ exp(logit)))
#The highest predicted mortality from sapsi in this data is 13%. As we know from our other models, this is false
```



**IV. Discussion**

The goal of our project was to assess whether a troupe of machine learning techniques offer any advantages to predicting length of stay, mortality, and pre-ICU conditions. Health professionals can benefit from advanced accurate predictions of ICU patient monitoring to adequately make stronger decisions on challenges in managerial health care. Poor management can result in patient rejection due to bed scarcity, loss of resources and manpower, and reduction in hospital revenue and quality of service (Schmidt et al 2013). Integrating machine learning models into intelligent ICU patient monitoring can have several beneficial implications. These implications were explored in three main questions: ICU length of stay, patient mortality, and pre-ICU conditions. Receiving more informed decision making in these areas can result in reductions in unwanted medical situations.

As we assumed, there was not a linear relationship between ICU length of stay and our variables of interest. Support vector regression worked well with our unstructured data, as did a random forest that fit our entire dataset. The models allowed us to observe variables that contributed differences to patients with increased length of stay. Variables such as the abg_count, age, and the service unit were statistically significant predictors of increased ICU stay. All models produced rmse values that indicated increases in prediction accuracy (in the scope of the range of our data, compared to random classification). Gaining full access to the dataset and filtering for patients that stayed beyond 5 days could allow exclusion of patients who passed away in the ICU and therefore more precise data points. A larger sample size would additionally assist in more accurate prediction. When split, our testing dataset for long length of stay contained 46 observations; to truly analyze the  performance of a model we would need more observations among long stay patients to see why they remain in the ICU for a longer period of time.

In a study similar to ours, researchers at the University of Pennsylvania predicted mortality for cancer patients within the Penn Medicine oncology practices.  Using random forest algorithms, this study observed 500 day mortality of 64.4% in the high risk group and 7.6% mortality in the low risk group (logarithmic regression and gradient boosting algorithms were also employed, but only results for the random forest model are published) (Parikh et al 2019). Our best mortality prediction model, using logistic regression, observed 2 year mortality of 70.5% in the high risk group and 15.9% in the low risk group. These models both aim to predict mortality in order to assist in providing the end of life care that the patient desires. Since classifying someone as in the high risk group increases the likelihood of introducing conversations surrounding end of life care, and therefore a choice to choose palliative care, we consider it more important to classify the high-risk group correctly.  As such, we consider our model to have outperformed Penn Medicine’s. 

We also view logistic regression as advantageous over other models due to the ease of modifying the threshold between “Censored” and “Death.”  In this report, we use the Youden index for this threshold in order to maximize both sensitivity and specificity, which resulted in a threshold of 0.5866, which means that observations had to have at least a 58.6% chance of survival to be classified by the model into the low-risk group.  Admittedly, this is a pretty high threshold for this classification; in the future, hospitals could choose a lower threshold in order to increase certainty for classification into the high risk group.
One important variable that we included in our model is the Simplified Acute Physiology Score (I)  predictor, which is formed from the collection of 14 physiological variables collected within 24 hours of admittance to the ICU to indicate the risk of death (Le Gall et al. 1984). While many of these variables are also included in the general dataset that we use in this paper, we keep SAPSI and view it as an interaction term between the variables that form the score. Using the formula by Le Gall et al. 1993, we mutate a SAPSI mortality prediction into our dataset.  Since the maximum predicted mortality derived through this formula was under 20%, we claim that our models outperform this score calculator.  However, due to the relative importance of SAPSI in predicting in random forests, we still support the collection of this metric for use in machine learning algorithms to predict mortality.

Our efforts to predict ICU pre-conditions with easily gathered medical metrics yield disappointing results. Both our logistic regression model and our random forest model had high accuracy (94.7% and 98.4% respectively) and specificity (98.4% and 99.8% respectively), but underwhelmed with low sensitivity scores (25.9% and 7.4% respectively). Catching positive cases of liver disease is important in an ICU setting where it can cause complications, so we hoped for a higher sensitivity. 

Fortunately, our results appear in line with similar research. Previous efforts to predict liver disease in patients achieved a positive predictive rate between 20% and 30%, similar to our logistic regression model (McLernon 2014). It is possible that liver disease is simply too difficult to predict with great certainty. However, the optimal decision threshold for our logistic regression model of 0.07 yielded a sensitivity of 70%. This result is exciting given the significant improvement in sensitivity compared to our other models, but the low decision threshold and lower specificity of 85% does make us wonder if the higher performance is reflective of a good model or merely an incredibly low decision threshold. Still, our results do suggest that there might be hope for doctors and researchers attempting to identify patients that might have liver disease or other comorbidities. 

Additionally, the limited MIMIC-II data that we were using only had 99 cases of liver disease. This may constrain our ability to properly train and test the model. Getting access to MIMIC-IV would greatly increase the amount of data we had to work with and increase the number of observations we could train and test our model on. Additionally, we only used two predictive methods for this research question. Focused study on predicting liver disease would allow for more predictive methods to be used. One of these other methods may prove superior to the ones we used. Finally, we selected live disease in part because it is a difficult comorbidity to diagnose. Applying our same work to a different comorbidity contained in the dataset might produce different results. Testing our process on different diseases would be an interesting extension of this research. In sum, we are unable to confidently predict comorbidities that ICU admittance may have with our current models; however, our work does suggest that this line of research could be fruitful if further pursued.  

As is with many real world applications, the best result from a model will depend on the data used. When wrangling data, it was crucial to extract covariates, but extraction of any variables must be done with caution because excluding relevant features in advance may generate sub optimal results. Furthermore, many variables interact with cloud data. For example, more severe patients tend to have longer lengths of  stay, but the sickest patients are also those at higher risk of death which will decrease the LOS. Having access to a full dataset, we could filter for patients who stayed beyond 5 days in the ICU, thus eliminating patients who passed shortly after admission, and examine what predicts longer stays in these patients. We additionally predicted mortality after two years; in many cases, hospitals are interested primarily in 6 month mortality. We also used the MIMIC database which represents a subset of a patient population from a singular hospital in Boston, and does not generalize to other populations or hospital systems in other areas across the United States and rest of the world. Additionally, machine learning algorithms to predict patient mortality typically utilize much larger patient databases with a wider array of variables.  Finally, our data contained no observations of sepsis, which is a primary target to either prevent or predict using machine learning since sepsis management is highly time sensitive in order to prevent mortality (Goh et al. 2021). It is unrealistic that there are incidences of sepsis at Beth Israel Hospital, so obtaining and training models on data containing sepsis is critical for future research to understand how sepsis affects patients in hospitals and the ICU. 

Future directions and extensions of this research  will involve extending the scope beyond Beth Israel Medical center, using holistic data to make broader optimal clinical intervention decisions. This research project thus lays a foundation for the future application of 3 patient related prediction models in ICU clinical practice.





**IV. Bibliography**
– Cite where you retrieved your data.

#References:

Alghatani, K., Ammar, N., Rezgui, A., & Shaban-Nejad, A. (2021). Predicting Intensive Care Unit Length of Stay and Mortality Using Patient Vital Signs: Machine Learning Model Development and Validation. JMIR medical informatics, 9(5), e21347. 

Evans, J.; Kobewka, D.; Thavorn, K.; D’Egidio, G.; Rosenberg, E.; Kyeremanteng, K. The impact of reducing intensive care unit length of stay on hospital costs: Evidence from a tertiary care hospital in Canada. Can. J. Anesth. J. Can. d’anesthésie 2018, 65, 627–635

Garcia, Alberto. “Critical care issues in the early management of severe trauma.” The Surgical clinics of North America vol. 86,6 (2006): 1359-87. doi:10.1016/j.suc.2006.07.004

Le Gall JR, Loirat P, Alperovitch A, et al. A simplified acute physiology score for ICU patients. Critical Care Medicine. 1984 Nov;12(11):975-977. DOI: 10.1097/00003246-198411000-00012. PMID: 6499483.

Marshall JC, Bosco L, Adhikari NK, Connolly B, Diaz JV, Dorman T, Fowler RA, Meyfroidt G, Nakagawa S, Pelosi P, Vincent JL, Vollman K, Zimmerman J. “What is an intensive care unit? A report of the task force of the World Federation of Societies of Intensive and Critical Care Medicine.” J Crit Care. 2017 Feb;37:270-276. doi: 10.1016/j.jcrc.2016.07.015. Epub 2016 Jul 25. PMID: 27612678.

Mayo Clinic. “Liver Disease.” Mayo Clinic. 

Orwelius, Lotti et al. “Pre-existing disease: the most important factor for health-related quality of life
	long-term after critical illness: a prospective, longitudinal, multicentre trial.” Critical care
(London, England) vol. 14,2 (2010): R67. doi:10.1186/cc8967

Parikh RB, Manz C, Chivers C, et al. Machine Learning Approaches to Predict 6-Month Mortality Among Patients With Cancer. JAMA Netw Open. 2019;2(10):e1915997.

Peres, I. T., Hamacher, S., Oliveira, F., Bozza, F. A., & Salluh, J. (2021). Prediction of intensive care units length of stay: a concise review. Predição do tempo de permanência em unidades de terapia intensiva: uma revisão concisa. Revista Brasileira de terapia intensiva, 33(2), 183–187. https://doi.org/10.5935/0103-507X.20210025

Rapoport J, Teres D, Lemeshow S, Avrunin JS, Haber R. Explaining variability of cost using a severity-of-illness measure for ICU patients. Med Care. Apr 1990;28(4):338-348.

Rapoport J, Teres D, Lemeshow S, Gehlbach S. A method for assessing the clinical performance and cost-effectiveness of intensive care units: a multicenter inception cohort study. Crit Care Med. Sep 1994;22(9):1385-1391.

Shaban-Nejad A, Michalowski M, Buckeridge DL. (2018) Health intelligence: how artificial intelligence transforms population and personalized health. NPJ Digit Med,1:53

Schmidt R, Geisler S, Spreckelsen C. Decision support for hospital bed management using adaptable individual length of stay estimations and shared resources. BMC Med Inform Decis Mak. 2013 Jan 07;13:3

Tomašev, N., Glorot, X., Rae, J.W. et al. A clinically applicable approach to continuous prediction of future acute kidney injury. Nature 572, 116–119 (2019)

Vincent, J.L.; Marshall, J.C.; Namendys-Silva, S.A.; Francois, B.; Martin-Loeches, I.; Lipman, J.; Reinhart, K.; Antonelli, M.; Pickkers, P.; Njimi, H.; et al. Assessment of the worldwide burden of critical illness: The Intensive Care Over Nations (ICON) audit. Lancet Resp. Med. 2014, 2, 380–386

World Health Organization: WHO Definition of Palliative Care. Published 2017. Accessed August 15th, 2017.

Wright  AA, Zhang  B, Ray  A,  et al.  Associations between end-of-life discussions, patient mental health, medical care near death, and caregiver bereavement adjustment.  JAMA. 2008;300(14):1665-1673


