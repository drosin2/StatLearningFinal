---
title: "Stats Learning Final Project"
author: "Jack Rudnick, Luke Lorenz, Doug Rosin"
date: "4/10/2022"
output: html_document
---


**I. Introduction**

– What does your dataset look like? What are the observations? What are the variables and their
types (categorical vs. quantitative)?


– Why is this data important and/or interesting, either to you personally or to someone else?


– If there is any background information that I should be aware of, please include it here. For
instance, if you have a jargon-heavy dataset, make sure that you describe it in a way that a
layperson would understand.


– What are your main research questions?


```{r importing libs}
library(tidyverse)
library(randomForest)
library(iml)
library(FNN)
library(e1071) #svm
library(pROC)
library(quantreg) #quant reg
library(vtable)
library(ISLR)
library(class)
library(sjlabelled)
library(corrplot)
library(cowplot)
```


```{r reading in dataset}
data <- read.csv("full_cohort_data.csv")
```
When our data is originally loaded in, the categorical vars are either 1 or 0. We need to convert the categorical vars to factors, so as not to be confused with our numeric variables of interest.


```{r convert to factors}
data <- data %>%
  mutate(gender_num = factor(gender_num,
                             levels = c(0,1),
                             labels = c("Female", "Male")),
         aline_flg = factor(aline_flg,
                             levels = c(0,1),
                             labels = c("Year", "No")),
         service_num = factor(service_num,
                              levels = c(0,1),
                              labels = c("MICU or FICU", "SICU")),
         icu_exp_flg = factor(icu_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         hosp_exp_flg = factor(hosp_exp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         day_28_flg = factor(day_28_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         censor_flg = factor(censor_flg,
                               levels = c(0,1),
                               labels = c("Death","Censored")),
         sepsis_flg = factor(sepsis_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         chf_flg = factor(chf_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         afib_flg = factor(afib_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         renal_flg = factor(renal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         liver_flg = factor(liver_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         copd_flg = factor(copd_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         cad_flg = factor(cad_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         stroke_flg = factor(stroke_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         mal_flg = factor(mal_flg,
                               levels = c(0,1),
                               labels = c("No","Yes")),
         resp_flg = factor(resp_flg,
                               levels = c(0,1),
                               labels = c("No","Yes"))
         )

```


```{r imputing NA values}
na_indicies <- which(is.na(data), arr.ind=TRUE)  


# Function to calculate the mode
Mode <- function(x) {
  xx <- unique(x)
  xx[which.max(tabulate(match(x, xx)))]
}


#replace the factor variables with the mode
for(i in 1:ncol(data)){
  for(j in 1:nrow(data)){
    if(class(data[[i]])== "numeric"){
      if (is.na(data[j,i])){
        data[j,i] <- median(as.numeric(unlist(data[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(data[j,i])){
        data[j,i]<-Mode(na.omit(data[,i]))
    }
  }
  }
}

```
We have ~1200 total vars that are NA, so we begin by replacing categorical NA with the mode of the column, and numeric NA's with the median. We then create a function that takes in a proximity matrix and updates imputed NA values 


```{r function to update NA values based on the prox matrix}
iterate <- function(proxMatrix){
  for (i in 1:nrow(na_indicies)){
  column_names <-colnames(data)
  # Storing indicies of the na value
  rowNum <- na_indicies[i,1]
  colNum <- na_indicies[i,-1]
  column_name <- column_names[colNum]
  # If the column contains numeric variables as opposed to categorical

  if(sapply(data[,colNum], class)[1] == "numeric"){
    # print("Entering col 5")
    #updtating NA val with imputed val across the column
    imputed_val <- sum(data[-rowNum,column_name] * proxMatrix[-rowNum,rowNum]/ sum(proxMatrix[-rowNum,rowNum]))
    data[rowNum,colNum] <- imputed_val
    
  }else{ #Contains categorical vars, not numeric
    # print("entering other col")
    #df to store proximity excluding the row of intrest
    df_factor <- data.frame(proximity = proxMatrix[-rowNum,rowNum],
                            class = data[-rowNum,column_name])
    
    val <- df_factor %>% #computing for mode
      group_by_at("class") %>%
      summarize(avg = mean(proximity))
    index <- which.max(val$avg)
    data[rowNum,column_name] <- val[index,1]
  }
}
  
  
}
```

Run the function above 10 times on our data
```{r imputing na data}
#only doing 10 iterations due to the large number of NA's
for (i in 1:10){
  #Fitting random forest to predict censor + getting prox mtx
  rf <- randomForest(formula = censor_flg ~., 
                   data = data,
                   proximity = TRUE)
  pmtx <- data.frame(rf$proximity)
  
  iterate(pmtx)
  print(paste("iteration", i , "done"))
}
```

**QUESTION OF INTEREST #1: PREDICTING LENGTH OF ICU STAY**

- Motivation:


#LINEAR REGRESSION FOR ICU STAY

For predicting the length of stay in the icu
  
```{r mean, median for ICU stay}
mean(data$icu_los_day)
median(data$icu_los_day)
```
Looking at some summary information, the average length of stay in the ICU was around 3.3 days, and the median was around 2.2 days. One can see that the mean is much higher than the median, indicating the data is skewed to the right. 

Continuing with some summary info, plotting brief exploratory quantile regression for age and ICU stay
```{r brief quantile regression for observation of age on icu stay}

colors <- c("#ffe6e6", "#ffcccc", "#ff9999", "#ff6666", "#ff3333",
            "#ff0000", "#cc0000", "#b30000", "#800000", "#4d0000", "#000000") #colors for regression lines
multi_rqfit <- rq(icu_los_day ~ age, data = data, tau = seq(0, 1, by = 0.1)) #quantile regrssion
plot(icu_los_day ~ age, data = data, pch = 16) #plotting reg
for (j in 1:ncol(multi_rqfit$coefficients)) { #plotting quartiles
    abline(coef(multi_rqfit)[, j], col = colors[j])
}
```
obviously, as the age of the patient admitted goes up, the average predicted ICU stay will also increase. This is apparent across all quantiles with all linear coefficients.

```{r mean, median, and mode for the length of ICU stay}
ggplot(data = data,
       aes(x = icu_los_day))+
  geom_histogram(binwidth = 0.5) +
  labs(x = "Length of Stay in the ICU (Days)")

```
Looking at the distribution of length of stays, you can see that there are many what people would consider outliers in the graph, there are numerous stays beyond 10 days

```{r maybe facet wrap based on a variable of intrest?}
ggplot(data = data,
       aes(x = icu_los_day))+
  geom_histogram(binwidth = 0.4) +
  labs(x = "Length of Stay in the Hospital (Days)") +
  facet_grid(~service_num)
```
When checking medical intensive care unit vs surgical intensive care unit, both seem to have similar distributions and a non-zero number of outliers. Therefore, this length of stay is not unique to the type of ICU that the patient was admitted into.

```{r boxplot for outliers}
#computing outlines
outlier_data <- data %>%
  mutate(outlier = icu_los_day > median(icu_los_day) + IQR(icu_los_day) * 1.5)

ggplot(data = outlier_data,
       aes(x = icu_los_day,
           y =  ""))+
  geom_boxplot() +
  labs(x = "Length of Stay in the ICU (Days)")
```
when looking at this data, we can see we have a large distribution of stays (1776) around the 2-4 day mark, but there are also 214 outliers in the data. These two are equally important to distinguish, as long stays in the ICU are resource intensive and important for allocation with finite spaces.

```{r getting the percent of outliers}
count(outlier_data %>% filter(outlier == TRUE))

count(outlier_data %>% filter(outlier == TRUE)) / count(outlier_data %>% filter(outlier == FALSE)) *100
```
These calculations show that 13.7 percent of our patients admitted into the ICU were outliers (214 patients in our dataset)

#FIT LR models on non-outliers, or short stay, vs outliers, or long stay
Because of the outliers above, we decided to split training data into outliers vs non outliers, then fit two linear models for short stay and long stay.When predicting for our data, we would then flow down our observation, first classfying if the data classified it as an outlier, then fitting the consequential linear model on that data. We found that this was a crude way to greatly improve the performance of a linear model on our dataset.

#For short stay
```{r splitting into testing/training short stay}
set.seed(1)

short_stay <- outlier_data %>% filter(outlier == FALSE)

n <- nrow(short_stay)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_short <- short_stay %>%
  slice(train_indices)
test_short <- short_stay %>%
  slice(-train_indices)

```

```{r}
#count unique vars, or columns that all contain the same variable and therefore will be excluded from our regression model
sapply(lapply(data, unique), length)
```
Checking for unique variables, we can see sepsis_flg only has one variable, so we do not include it in our model fitting

```{r Linear regression model for shorter stay (on training)}
#Taking a linear model without sepsis, because that column has no unique values and will result in error if run
m <- lm(formula = icu_los_day ~ ., data = subset(short_stay,
                                                 select = -sepsis_flg)) 
m
```

```{r making predictions on the test data}
pred_short_stay <- predict(m, newdata = test_short)
```

```{r compute MSE and RMSE}
mean( (test_short$icu_los_day - pred_short_stay)^2 ) # test MSE
sqrt( mean( (test_short$icu_los_day - pred_short_stay)^2 ) ) # test RMSE
```
How do these compare to our longer stay model?


#Repeating the steps for longer stay
```{r Linear regression model for longer stay (on training)}
set.seed(1)
#filtering by outliers
long_stay <- outlier_data %>% filter(outlier == FALSE)

n <- nrow(long_stay)
set.seed(10)
# 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_long <- long_stay %>%
  slice(train_indices)
test_long <- long_stay %>%
  slice(-train_indices)

```


```{r}
#same procedure as fitting our linear model for short stay
m2 <- lm(formula = icu_los_day ~ ., data = subset(test_long,
                                                 select = -sepsis_flg)) 
m2
```

```{r predicting using our data}
pred_long_stay <- predict(m2, newdata = test_long)
```

```{r compute MSE and RMSE}
mean( (test_long$icu_los_day - pred_long_stay)^2 ) # test MSE
sqrt( mean( (test_long$icu_los_day - pred_long_stay)^2 ) ) # test RMSE
```
comparatively, these are two very small error rates. For a test observation, we will therefore calculate if it is an outlier as we did above, and filter that result down into an appropriate linear model of prediction.
How do these rates compare to if we were to fit only 1 model on the dataset?

#Testing on a singluar model for RMSE value comparison
```{r testing without outlier filtering (both short and long stays)}
set.seed(1)

n <- nrow(data)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_compare <- data %>%
  slice(train_indices)
test_compare <- data %>%
  slice(-train_indices)
```

```{r testing without outlier filtering}
m3 <- lm(formula = icu_los_day ~ ., data = subset(test_compare,
                                                 select = -sepsis_flg))
pred_long_stay <- predict(m3, newdata = test_compare)

#computing
mean( (test_long$icu_los_day - pred_long_stay)^2 ) # test MSE
sqrt( mean( (test_long$icu_los_day - pred_long_stay)^2 ) ) # test RMSE
```
Without splitting the observations into long stay vs short stay, our linear model does not work nearly as well as our two smaller models



#FITTING RANDOM FOREST for ICU STAY

How does a random forest compare to our linear model? Random forests additionally allow for exploration into important variables
```{r splitting testing/training again}
set.seed(1)

n <- nrow(data)
set.seed(10)
# Common to use 70% or 80% of observations for training set
train_indices <- sample(1:n, size = floor(0.8*n)) 
train_rf <- data %>%
  slice(train_indices)
test_rf <- data %>%
  slice(-train_indices)

```

Filtering out variable with obvious covariance (length of stay in hospital)
```{r fitting random forest without the length of stay in the hospital}
updated_train_rf <- train_rf %>%
  dplyr::select(-hospital_los_day) #taking out length of stay in hospital???
#removing var with obvious covariance

rf <- randomForest(formula = icu_los_day ~., 
                   data = train_rf,
                   importance = TRUE)
#This is the one we're currently using , DOES NOT contain hospital_los_day
rf_with_hospital <- randomForest(formula = icu_los_day ~., 
                   data = updated_train_rf,
                   importance = TRUE)
```

```{r predicting w test data and fitted forest}
pred_rf <- predict(rf_with_hospital, newdata = test_rf)
```

```{r computing accuracy compared to our split linear models}
mean( (test_rf$icu_los_day - pred_rf)^2 ) # test MSE
sqrt( mean( (test_rf$icu_los_day - pred_rf)^2 ) ) # test RMSE
```
our rmse was a bit higher for the random forest, but we also didn't split apart into outliers. 

```{r variable importance plot}
varImpPlot(rf_with_hospital, cex = 0.7)
```
arterial blood gas count seemed to be the most important predictor for predicting length of ICU stay. An arterial blood gas analysis (ABG) measures the balance of oxygen and carbon dioxide in your blood to see how well your lungs are working

```{r partial dependence plot}
pred_obj <- Predictor$new(rf, data = data)
temp <- FeatureEffect$new(pred_obj, feature = "abg_count", method = "pdp")
plot(temp)
```
The length of stay increases significantly as the abg count increases. For the vast majority of ICU patients, their abg count is less than 15, meaning the influcence on ICU stay is relatively low. However, when the abg count exceeds this, predicted length of ICU stay climbs steeply, as between 15-30 the length of stay jumps 2 days.


```{r}
pred_obj <- Predictor$new(rf, data = data)
temp <- FeatureEffect$new(pred_obj, feature = "resp_flg", method = "pdp")
plot(temp)
```
many other variables additionally had impacts on decision. Though less severe than abg count, resp_flg (respiratory infection) (among other variables) also resulted in a small increase in the predicted length of stay.


#Predicting Liver Disease

```{r}
table(data$liver_flg)

99/1677 *100
```
5.90% of patients in the data set had liver disease. This likely means sensitivity will be off. 

```{r select data to predict patients with liver disease}
data_liver_disease <- data %>%
  select(age, weight_first, bmi, map_1st, hr_1st, temp_1st, spo2_1st
         , wbc_first, hgb_first,
         platelet_first, 
         sodium_first,
         potassium_first,
         tco2_first,
         chloride_first,
         bun_first,
         creatinine_first,
         po2_first,
         pco2_first, 
         liver_flg)
```

selecting for things that can be quickly tested for when a patient is admitted to the ICU or info than easily be gotten by docotors. 

#liver disease method one: logistic regression
```{r split into test and train for liver disease}
p <- nrow(data_liver_disease)
set.seed(1)
# Common to use 70% or 80% of observations for training set
train_liver_indices <- sample(1:p, size = floor(0.7*p)) 
train_liver_data <- data_liver_disease %>%
  slice(train_liver_indices)
test_liver_data <- data_liver_disease %>%
  slice(-train_liver_indices)
```

```{r}
z <- glm(formula = liver_flg ~., data = train_liver_data, family = binomial)
summary(z)
```

some significant predictors here. 

```{r}
pred_prob_liver <- predict(z, newdata = test_liver_data, type = "response")
```


```{r use a loop to determine best decision threshold}

store_accuracy <- c()
store_sensitivity <- c()
store_specificity <- c()

for( t in seq(from = 0.1, to = 0.9, by = 0.1)) {
  pred_class_labels_liver <- ifelse(pred_prob_liver > t, "Yes", "No")
  conf_mtx_liver <- table(pred_class_labels_liver, test_liver_data$liver_flg)
  
  #accuracy
  store_accuracy[t*10] <- sum(diag(conf_mtx_liver))/sum(conf_mtx_liver)
  #sensitivity (true liver disease haves labeled as liver disease haves)
  store_sensitivity[t*10] <- conf_mtx_liver[2,2]/sum(conf_mtx_liver[,2])
  #specificity(people that do not have liver disease that were predicted as not having liver    disease)
  store_specificity[t*10] <- conf_mtx_liver[1,1]/sum(conf_mtx_liver[,1])
  
  
}

tibble(decision_threshold = seq(from = 0.1, to = 0.9, by = 0.1),
       accuracy = store_accuracy,
       sensitivity = store_sensitivity,
       specificity = store_specificity)


```

Im selecting a decision threshold of 0.4. This is getting a high sensitivity without sacrificing much in the way of accuracy or specificity. 

```{r}
pred_class_labels_liver <- ifelse(pred_prob_liver > 0.4, "Yes", "No") # convert to labels

conf_mtx_liver <- table(pred_class_labels_liver, test_liver_data$liver_flg)
conf_mtx_liver
```

The model seems good at predicting patients without liver disease and bad a predicting patients with liver disease. This is worrying given that a doctor wants to see higher sensativity in these cases.  

```{r}
#accuracy
sum(diag(conf_mtx_liver))/sum(conf_mtx_liver)
#sensitivity (true liver disease haves labeled as liver disease haves)
conf_mtx_liver[2,2]/sum(conf_mtx_liver[,2])
#specificity(people that do not have liver disease that were predicted as not having liver disease)
conf_mtx_liver[1,1]/sum(conf_mtx_liver[,1])
```

```{r}
roc_obj_liver <- roc(predictor = pred_prob_liver, 
               response = test_liver_data$liver_flg)
plot(roc_obj_liver, legacy.axes = TRUE)
```

```{r}
auc(roc_obj_liver)


coords(roc_obj_liver) %>%
  mutate(youden = sensitivity + specificity - 1) %>%
  slice_max(youden)
```

AUC and youden are both fine, but I think this is from the high specificity and does not really refelct how bad the model is at finding cases of liver disease. 

#random forest to see importance on liver disease
`
```{r random forest to determine predictor importance on liver disease}

rf_liver <- randomForest(formula =  liver_flg~ ., #for importance of liver disease
                   data = data_liver_disease,
                   importance = TRUE)
```

```{r}
varImpPlot(rf_liver, cex = 0.7)
```


```{r}
set.seed(6)

rf_prediction_liver <- randomForest(formula =  liver_flg~ ., #for importance of liver disease
                   data = train_liver_data,
                   importance = TRUE)
```

```{r}
head(rf_prediction_liver$predicted)
head(rf_prediction_liver$votes)
rf_prediction_liver$confusion
rf_prediction_liver$err.rate

plot(1:500, rf_prediction_liver$err.rate[,3])
```

```{r}
pred_apps <- predict(rf_prediction_liver, newdata = test_liver_data)
pred_apps
```

```{r}
rf_liver_tests <- tibble(predicted = pred_apps,
       actual = test_liver_data$liver_flg)
```

```{r}
cf <- table(rf_liver_tests)

#accuracy
sum(diag(cf))/sum(cf)
#sensitivity (true liver disease haves labeled as liver disease haves)
cf[2,2]/sum(cf[,2])
#specificity(people that do not have liver disease that were predicted as not having liver disease)
cf[1,1]/sum(cf[,1])
```

Basically we see the same thing here as we did in logistic regression. The model is really good at predicting patients that do not have liver disease and really bad at predicting cases with liver disease. I think this is likely due to a lack of observations with liver disease to train against.   

```{r}
importance(rf)
```

```{r}
#variable importance plot
varImpPlot(rf, cex = 0.5)
```

It appears cp and ca are important under both MDI and MDA

```{r}
age_obj <- Predictor$new(rf, data = data)
age_plot <- FeatureEffect$new(age_obj, feature = "age", method = "pdp", center.at = 0)
plot(age_plot)
```



```{r}
bun_obj <- Predictor$new(rf, data = data)
bun_plot <- FeatureEffect$new(bun_obj, feature = "bun_first", method = "pdp", center.at = 0)
plot(bun_plot)
```

```{r}
los_obj <- Predictor$new(rf, data = data)
bun_plot <- FeatureEffect$new(los_obj, feature = "sapsi_first", method = "pdp", center.at = 0)
plot(bun_plot)
```
```{r}
importance(rf)
varImpPlot(rf, cex =0.5)
```


###MORTALITY PREDICTION

```{r}
set.seed(1)
mortalitydata <- data
```

```{r}
num_cols <- unlist(lapply(mortalitydata, is.numeric))
```

```{r}
data.knn <- mortalitydata[ , num_cols] %>%
  dplyr::select(where(is.numeric))
```


```{r}
data.knn$censor_flg <- mortalitydata$censor_flg
```


## Split Into Training vs. Test

```{r split into training and test}
#set.seed(1)
n <- nrow(data.knn)
train_indices <- sample(1:n, size = floor(0.8*n))
train.knn <- data.knn %>% slice(train_indices)
test.knn <- data.knn %>% slice(-train_indices)
```

## k-Fold Cross-Validation

```{r assign observations to folds}
num_folds <- 10 # number of folds

# Create a variable (`fold_id`) assigning each observation in the training
# set to a fold between 1 and `num_folds`
train.knn <- train.knn %>%
  mutate(fold_id = 1:n() %% num_folds)
```

```{r k-fold CV}
# Vector of numbers of nearest neighbors 
K_seq <- 1:60

# Create vector to store k-fold cross-validation error for each number of
# nearest neighbors
store_kCV_err <- c()

# Loop over numbers of nearest neighbors
for(num_near in K_seq) {
  
  # Create vector to store model error for each held-out fold
  store_fold_err <- rep(0, num_folds)
  
  # Loop over folds
  # Remember that `fold_id` goes from 0 to (number of folds - 1)
  for(i in 0:(num_folds-1)) { 
    
    # Train on all but the ith fold
    # Separate into predictors (X) and response (Y)
    train_X <- train.knn %>% 
      filter(fold_id != i) %>% 
      select(-censor_flg, -fold_id) 
    train_Y <- train.knn %>% 
      filter(fold_id != i) %>% 
      pull(censor_flg) 
    
    # Test on the ith fold
    # Separate into predictors (X) and response (Y)
    test_X <- train.knn %>% 
      filter(fold_id == i) %>% 
      select(-censor_flg, -fold_id) 
    test_Y <- train.knn %>%
      filter(fold_id == i) %>%
      pull(censor_flg)
    
    # Normalize the predictors in the training and test sets, using the means and
    # standard deviations from the training set only
    train_means <- apply(train_X, MARGIN = 2, FUN = mean)
    train_sds <- apply(train_X, MARGIN = 2, FUN = sd)
    train_X_norm <- scale(train_X)
    test_X_norm <- scale(test_X, center = train_means, scale = train_sds)
    
    # Run KNN classification with the given number of nearest neighbors
    knn_preds <- knn(train = train_X_norm, 
                     test = test_X_norm, 
                     cl = train_Y, 
                     k = num_near)
    
    # Store the model error for this fold
    # The model error is the proportion of incorrectly classified
    # observations in the hold-out fold
    store_fold_err[i+1] <- sum(knn_preds != test_Y)/length(test_Y)
    
  } 
  
  # Store the k-fold CV error for this number of nearest neighbors
  # The k-fold CV error is the average model error across all the held-out
  # folds
  store_kCV_err <- c(store_kCV_err, mean(store_fold_err))
  
}
```

```{r plot k-fold CV error as function of K}
# Plot the k-fold CV error as a function of the number of nearest neighbors
ggplot(tibble(K = K_seq, err = store_kCV_err), aes(x = K, y = err)) +
  geom_point() +
  geom_line() +
  labs(title = "K-Fold Cross-Validation Error",
       subtitle = "KNN Classification",
       x = "Number of Nearest Neighbors",
       y = "Average Proportion of Misclassified Observations")
```

## Fit Final Model

```{r choose value of K}
# Choose optimal value of K based on error plot above
# Note that this is somewhat subjective!
opt_num_near <- 19
```

```{r fit final model}
# Fit the final model on the full training set, using the optimal K chosen
# above

# Separate full training set into predictors (X) and response (Y)
train_X <- train.knn %>% 
  select(-censor_flg, -fold_id) 
train_Y <- train.knn %>% 
  pull(censor_flg) 

# Separate test set into predictors (X) and response (Y)
test_X <- test.knn %>% 
  select(-censor_flg) 
test_Y <- test.knn %>%
  pull(censor_flg)

# Normalize the predictors in the training and test sets, using the means and
# standard deviations from the training set only
train_means <- apply(train_X, MARGIN = 2, FUN = mean)
train_sds <- apply(train_X, MARGIN = 2, FUN = sd)
train_X_norm <- scale(train_X)
test_X_norm <- scale(test_X, center = train_means, scale = train_sds)

knn_preds_final <- knn(train = train_X_norm, 
                       test = test_X_norm, 
                       cl = train_Y, 
                       k = opt_num_near) # use optimal K
```

```{r assess final model performance}
# Make confusion matrix
conf_mtx <- table(knn_preds_final, test_Y)
conf_mtx

sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # sensitivity
conf_mtx[1,1]/sum(conf_mtx[,1]) # specificity
```



###LOGISTIC REGRESSION ON DEATH



#```{r}
num_cols <- unlist(lapply(data, is.numeric))
#```

#```{r}
data.glm <- data[ , num_cols] 
#```


#```{r}
data.glm$censor_flg <- data$censor_flg
#```



#```{r convert to factors}
data <- data %>%
  mutate(gender_num = factor(gender_num,
                             levels = c(0,1)),
                             #labels = c("Female", "Male")),
         aline_flg = factor(aline_flg,
                             levels = c(0,1)),
                             #labels = c("Year", "No")),
         service_num = factor(service_num,
                              levels = c(0,1)),
                              #labels = c("MICU or FICU", "SICU")),
         icu_exp_flg = factor(icu_exp_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         hosp_exp_flg = factor(hosp_exp_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         day_28_flg = factor(day_28_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         censor_flg = factor(censor_flg,
                               levels = c(0,1)),
                               #labels = c("Death","Censored")),
         sepsis_flg = factor(sepsis_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         chf_flg = factor(chf_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         afib_flg = factor(afib_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         renal_flg = factor(renal_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         liver_flg = factor(liver_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         copd_flg = factor(copd_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         cad_flg = factor(cad_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         stroke_flg = factor(stroke_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         mal_flg = factor(mal_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes")),
         resp_flg = factor(resp_flg,
                               levels = c(0,1)),
                               #labels = c("No","Yes"))
         )
#```

Question 1: Cleaned dataset
```{r}
#Split into training and test set
set.seed(1)
n <- nrow(data)
train_indices <- sample(1:n, size = floor(0.8*n))
train_data <- data %>%slice(train_indices)
test_data <- data %>%slice(-train_indices)
```

```{r imputing NA values for training set}
na_indicies <- which(is.na(data), arr.ind=TRUE)  
# Function to calculate the mode
Mode <- function(x) {
  xx <- unique(x)
  xx[which.max(tabulate(match(x, xx)))]
}
#replace the factor variables with the mode
for(i in 1:ncol(train_data)){
  for(j in 1:nrow(train_data)){
    if(class(train_data[[i]])== "numeric"){
      if (is.na(train_data[j,i])){
        train_data[j,i] <- mean(as.numeric(unlist(train_data[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(train_data[j,i])){
        train_data[j,i]<-Mode(na.omit(train_data[,i]))
    }
  }
  }
}
```

```{r imputing NA values for test set}
#na_indicies <- which(is.na(data), arr.ind=TRUE)  
# Function to calculate the mode
Mode <- function(x) {
  xx <- unique(x)
  xx[which.max(tabulate(match(x, xx)))]
}
#replace the factor variables with the mode
for(i in 1:ncol(test_data)){
  for(j in 1:nrow(test_data)){
    if(class(test_data[[i]])== "numeric"){
      if (is.na(test_data[j,i])){
        test_data[j,i] <- mean(as.numeric(unlist(train_data[,i])),
                            na.rm= TRUE)
      }
    }
    else{
      if (is.na(test_data[j,i])){
        test_data[j,i]<-Mode(na.omit(train_data[,i]))
    }
  }
  }
}
```





```{r}
train_data<- train_data %>%
  select(-day_icu_intime, -mort_day_censored, -day_28_flg, -hosp_exp_flg, -icu_exp_flg, -hospital_los_day, -service_unit)
test_data<- test_data %>%
  select(-day_icu_intime, -mort_day_censored, -day_28_flg, -hosp_exp_flg, -icu_exp_flg, -hospital_los_day, -service_unit)

```









**10/10**
Part 2: Fit logistic regression
```{r}
m <- glm(formula = censor_flg ~ ., 
          data = subset(train_data,select = -sepsis_flg), 
          family = binomial)
summary(m)
```



**5/5**
Question 3: Model Evaluation

```{r}
#Calculate the probability of fault in the test set
pred_prob <- predict(m, newdata = test_data, type = "response") # predicted probabilities

# roc() function to plot an ROC curve for the test set
roc_obj <- roc(predictor = pred_prob, 
               response = test_data$censor_flg)
plot(roc_obj, legacy.axes = TRUE)
```

```{r}
auc(roc_obj)
```
This is somewhat better than a model that randomly guesses.


```{r}
#find youden index and ideal decision threshold
coords(roc_obj) %>%
  mutate(youden = sensitivity + specificity - 1) %>%
  slice_max(youden)
```
The ROC curve looks like a staircase because there are only 4 positives in the test set. Lowering the decision threshold slightly usually affects the specificity (by adding false positives) without changing the sensitivity (no additional true positives). In other words, when the model correctly classifies a positive, the sensitivity jumps. The height of the jumps are all the same because each one is a 25% increase in the true positive rate. The widths of the steps correspond to the number of false positives with a predicted probability between each true positive. This number of false positives is not consistent which is why the steps have differing widths.

```{r}
#Use the ideal decision threshold from the Youden Index calculation
pred_class_labels <- ifelse(pred_prob > 0.55057, "Yes", "No") # use the ideal threshold
#make a confusion matrix
conf_mtx <- table(pred_class_labels, test_data$censor_flg)
conf_mtx

```

```{r}
sum( diag(conf_mtx) )/sum(conf_mtx) # accuracy
conf_mtx[2,2]/sum(conf_mtx[,2]) # specificity
conf_mtx[1,1]/sum(conf_mtx[,1]) # sensitivity
```


```{r}
library(cowplot)
age<- ggplot(train_data, aes(x=age, y=censor_flg)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial)) +
  labs(x = "Age")
copd_flg<- ggplot(train_data, aes(x=copd_flg, y=censor_flg)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))+
  labs (x = "Chronic Obstructive Pulmonary Disease")
stroke_flg<- ggplot(train_data, aes(x=stroke_flg, y=censor_flg)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial)) +
  labs(x = "Stroke")
chloride_first<- ggplot(train_data, aes(x=chloride_first, y=censor_flg)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial)) +
  labs(x = "First Chloride")

plot_grid(age, chloride_first, copd_flg, stroke_flg )
```


**III. Discussion/Conclusion**

– What did you learn about your data from this project?
  -
– What would be the next step if you or someone else were to continue analyzing this data set? In
answering this, you can assume that you have unlimited time and resources to gather more data.
  -Full dataset
  -Other hospitals
  -
– What are some of the limitations of your work? For example, maybe you think you need other
modeling tools, maybe you think there’s too much randomness to glean anything meaningful from
your data, etc.
  -


**IV. Bibliography**
– Cite where you retrieved your data.
– If you did any background research on your topic (e.g., to figure out what a variable means or to
verify whether your results make sense), include those sources too.

